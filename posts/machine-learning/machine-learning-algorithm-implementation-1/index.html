<!DOCTYPE html>
<html lang="en-us">

  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <meta name="author" content="Zhengyu Chen">
    <meta name="description" content="http://zintrulcre.vip">
    <meta name="keywords" content="ZintrulCre">
    
    <meta property="og:site_name" content="ZintrulCre">
    <meta property="og:title" content="
  Machine Learning Algorithm Implementation (1): Linear Regression - ZintrulCre
">
    <meta property="og:description" content="">
    <meta property="og:type" content="website">
    <meta property="og:url" content="http://zintrulcre.vip/posts/machine-learning/machine-learning-algorithm-implementation-1/">
    <meta property="og:image" content="http://zintrulcre.vip">
    <meta name="twitter:card" content="summary">
    <meta name="twitter:site" content="http://zintrulcre.vip/posts/machine-learning/machine-learning-algorithm-implementation-1/">
    <meta name="twitter:image" content="http://zintrulcre.vip">

    <base href="http://zintrulcre.vip">
    <title>
  Machine Learning Algorithm Implementation (1): Linear Regression - ZintrulCre
</title>

    <link rel="canonical" href="http://zintrulcre.vip/posts/machine-learning/machine-learning-algorithm-implementation-1/">
    
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.2.0/css/all.css" integrity="sha384-hWVjflwFxL6sNzntih27bfxkr27PmbbK/iSvJ+a4+0owXq79v+lsFkW54bOGbiDQ" crossorigin="anonymous">
    
    <link  rel="stylesheet" href="https://fonts.googleapis.com/css?family=Fira+Mono:400,700">
    <link rel="stylesheet" href="//cdn.rawgit.com/necolas/normalize.css/master/normalize.css">
    <link rel="stylesheet" href="http://zintrulcre.vip/css/style.min.css">

    

    

    <link rel="icon" type="image/png" href="http://zintrulcre.vip/images/favicon-32x32.png" sizes="32x32">
    <link rel="icon" type="image/png" href="http://zintrulcre.vip/images/favicon-16x16.png" sizes="16x16">

    

    <meta name="generator" content="Hugo 0.53" />
  </head>

  <body class="">
    <main class="wrapper">
      <nav class="navigation">
  <section class="container">
    <a class="navigation-title" href="/">ZintrulCre</a>
    <input type="checkbox" id="menu-control"/>
    <label class="menu-mobile  float-right " for="menu-control">
      <span class="btn-mobile  float-right ">&#9776;</span>
      <ul class="navigation-list">
        
          
            <li class="navigation-item  align-center ">
              <a class="navigation-link" href="http://zintrulcre.vip/posts/">Posts</a>
            </li>
          
            <li class="navigation-item  align-center ">
              <a class="navigation-link" href="http://zintrulcre.vip/categories">Categories</a>
            </li>
          
            <li class="navigation-item  align-center ">
              <a class="navigation-link" href="http://zintrulcre.vip/tags/">Tags</a>
            </li>
          
            <li class="navigation-item  align-center ">
              <a class="navigation-link" href="http://zintrulcre.vip/about">About</a>
            </li>
          
        
        
      </ul>
    </label>
  </section>
</nav>


<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.6.0/katex.min.css">
<script src="//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.6.0/katex.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.6.0/contrib/auto-render.min.js"></script>

      <div class="content">
        
  <section class="container post">
  <article>
    <header>
      <h1 class="title">Machine Learning Algorithm Implementation (1): Linear Regression</h1>
      <h2 class="date">January 30, 2019</h2>

      
    </header>

    <p>Click <a href="https://github.com/ZintrulCre/Machine-Learning/blob/master/Linear%20Regression.ipynb">here</a> to see the implementation in Jupyter Notebook</p>

<h1 id="linear-regression">Linear regression</h1>

<ul>
<li>Goal

<ul>
<li>fit a linear model, using only the <code>numpy</code> library</li>
<li>visualize the output, using the <code>matplotlib</code> library</li>
</ul></li>
<li>Implementation Methods

<ul>
<li>iterative updates (coordinate descent)</li>
<li>linear algebra</li>
</ul></li>
<li>Verification

<ul>
<li>compare our output to the output of using library <code>sklearn</code>.</li>
</ul></li>
</ul>

<h2 id="1-import-library">1. Import Library</h2>

<p>Firstly we should import the relevant libraries (<code>numpy</code>, <code>matplotlib</code>, etc.).</p>
<div class="highlight"><pre class="chroma">import numpy as np
import matplotlib.pyplot as plt
import io</pre></div>
<h2 id="2-formula">2. Formula</h2>

<p>The linear model can be expressed as:</p>

<p><span  class="math">\[y = w_0 + \sum_{j = 1}^{m} w_j x_j = \mathbf{w} \cdot \mathbf{x}\]</span></p>

<p>where</p>

<ul>
<li><span  class="math">\(y\)</span> is the <strong>target variable</strong></li>
<li><span  class="math">\(\mathbf{x} = [x_1, \ldots, x_m]\)</span> is a vector of <strong>features</strong> (<span  class="math">\(x_0 = 1\)</span>)</li>
<li><span  class="math">\(\mathbf{w} = [w_0, \ldots, w_m]\)</span> are the <strong>weights</strong>.</li>
</ul>

<p>To fit the model, we should <strong>minimise</strong> the empirical risk with respect to <span  class="math">\(\vec{w}\)</span>.</p>

<p>In this case, this amounts to minimising the sum of squared residuals (square loss):</p>

<p><span  class="math">\[SSR(\mathbf{w}) = \sum_{i=1}^{n}(y_i - \mathbf{w} \cdot \mathbf{x}_i)^2\]</span></p>

<p>For simplicity, we only consider the case <span  class="math">\(m = 1\)</span> (i.e. only one feature excluding the intercept).</p>

<h2 id="3-import-data-set">3. Import Data set</h2>

<p>Let's use some data of the gold medal race times for marathon winners from 1896 to 2012 of Olympics.</p>

<p>The code block below reads the data into a numpy array of floats, and prints the result.</p>
<div class="highlight"><pre class="chroma"># CSV file with variables YEAR,TIME
csv =&#34;&#34;&#34;
1896,4.47083333333333
1900,4.46472925981123
1904,5.22208333333333
1908,4.1546786744085
1912,3.90331674958541
1920,3.5695126705653
1924,3.8245447722874
1928,3.62483706600308
1932,3.59284275388079
1936,3.53880791562981
1948,3.6701030927835
1952,3.39029110874116
1956,3.43642611683849
1960,3.2058300746534
1964,3.13275664573212
1968,3.32819844373346
1972,3.13583757949204
1976,3.07895880238575
1980,3.10581822490816
1984,3.06552909112454
1988,3.09357348817
1992,3.16111703598373
1996,3.14255243512264
2000,3.08527866650867
2004,3.1026582928467
2008,2.99877552632618
2012,3.03392977050993
&#34;&#34;&#34;

# Read into a numpy array as floats
olympics = np.genfromtxt(io.BytesIO(csv.encode()), delimiter=&#34;,&#34;)
print(olympics)</pre></div>
<p>Now we take the race time as the <strong>target variable</strong> <span  class="math">\(y\)</span> and the year of the race as the only <strong>feature</strong> <span  class="math">\(x = x_1\)</span>.</p>
<div class="highlight"><pre class="chroma">x = olympics[:, 0:1]
y = olympics[:, 1:2]</pre></div>
<p>Plot <span  class="math">\(y\)</span> and <span  class="math">\(x\)</span>.</p>
<div class="highlight"><pre class="chroma">plt.plot(x, y, &#39;rx&#39;)
plt.ylabel(&#34;y (Race time)&#34;)
plt.xlabel(&#34;x (Year of race)&#34;)
plt.show()</pre></div>
<p><figure><img src="https://raw.githubusercontent.com/ZintrulCre/zintrulcre.github.io/master/data/Machine-Learning/Machine-Learning-Algorithm-Implementation-1/1.png" alt="1"></figure></p>

<p>We can see that a linear model could be a decent fit for this data.</p>

<h2 id="4-iterative-solution-coordinate-descent">4. Iterative solution (coordinate descent)</h2>

<p>Expanding out the sum of square residuals for this simple case (where <span  class="math">\(\mathbf{w}=[w_0, w_1]\)</span>) we have:</p>

<p><span  class="math">\[SSR(w_0, w_1) = \sum_{i=1}^{n}(y_i - w_0 - w_1 x_i)^2\]</span></p>

<p>Let's start with an initial guess for the slope <span  class="math">\(w_1\)</span> (which is clearly negative from the plot).</p>
<div class="highlight"><pre class="chroma">w1 = -0.5</pre></div>
<p>Then using the maximum likelihood update, we can get the following estimate for the intercept <span  class="math">\(w_0\)</span></p>

<p><span  class="math">\[w_0 = \frac{\sum_{i=1}^{n}(y_i-w_1 x_i)}{n}\]</span></p>
<div class="highlight"><pre class="chroma">def update_w0(x, y, w1):
    return (y - w1*x).mean()

w0 = update_w0(x, y, w1)
print(w0)</pre></div>
<p>Similarly, we can update <span  class="math">\(w_1\)</span> based on this new estimate of <span  class="math">\(w_0\)</span></p>

<p><span  class="math">\[w_1 = \frac{\sum_{i=1}^{n} (y_i - w_0) \times x_i}{\sum_{i=1}^{n} x_i^2}\]</span></p>
<div class="highlight"><pre class="chroma">def update_w1(x, y, w0):
    return ((y - w0)*x).sum()/(x**2).sum()

w1 = update_w1(x, y, w0)
print(w1)</pre></div>
<p>Now let's examine the quality of fit for these values for the weights <span  class="math">\(w_0\)</span> and <span  class="math">\(w_1\)</span>. We create a vector of &quot;test&quot; values <code>x_test</code> and a function to compute the predictions according to the model.</p>
<div class="highlight"><pre class="chroma">x_test = np.arange(1890, 2020)[:, None]

def predict(x_test, w0, w1): 
    return w0 + w1 * x_test

y_test = predict(x_test, w0, w1)
print(y_test)</pre></div>
<p>Now plot the test predictions with a blue line on the same plot as the data.</p>
<div class="highlight"><pre class="chroma">def plot_fit(x_test, y_test, x, y): 
    plt.plot(x_test, y_test, &#39;b-&#39;)
    plt.plot(x, y, &#39;rx&#39;)
    plt.ylabel(&#34;y (Race time)&#34;)
    plt.xlabel(&#34;x (Year of race)&#34;)
    plt.show()

plot_fit(x_test, predict(x_test, w0, w1), x, y)</pre></div>
<p><figure><img src="https://raw.githubusercontent.com/ZintrulCre/zintrulcre.github.io/master/data/Machine-Learning/Machine-Learning-Algorithm-Implementation-1/2.png" alt="2"></figure></p>

<p>We can compute the sum of square residuals <span  class="math">\(SSR(w_0,w_1)\)</span> on the training set to measure the goodness of fit.</p>
<div class="highlight"><pre class="chroma">def compute_SSR(x, y, w0, w1): 
    return ((y - w0 - w1*x)**2).sum()
    
print(compute_SSR(x, y, w0, w1))</pre></div>
<p>It's obvious that the fit isn't very good from the plot.</p>

<p>We should repeat the alternating parameter updates many times before the algorithm converges to the optimal weights.</p>
<div class="highlight"><pre class="chroma">for i in np.arange(10000):
    w1 = update_w1(x, y, w0) 
    w0 = update_w0(x, y, w1) 
    if i % 500 == 0:
        print(&#34;Iteration #{}: SSR = {}&#34;.format(i, compute_SSR(x, y, w0, w1)))
print(&#34;Final estimates: w0 = {}; w1 = {}&#34;.format(w0, w1))</pre></div>
<p>Let's try plotting the result again.</p>
<div class="highlight"><pre class="chroma">plot_fit(x_test, predict(x_test, w0, w1), x, y)</pre></div>
<p><figure><img src="https://raw.githubusercontent.com/ZintrulCre/zintrulcre.github.io/master/data/Machine-Learning/Machine-Learning-Algorithm-Implementation-1/3.png" alt="3"></figure></p>

<h2 id="5-linear-algebra-solution">5. Linear algebra solution</h2>

<p>it's possible to solve for the optimal weights <span  class="math">\(\mathbf{w}^\star\)</span> analytically. The solution is</p>

<p><span  class="math">\[\mathbf{w}^* = \left[\mathbf{X}^\top \mathbf{X}\right]^{-1} \mathbf{X}^\top \mathbf{y}\]</span></p>

<p>where</p>

<p><span  class="math">\[\mathbf{X} = \begin{pmatrix} 1 & x_1 \\ 1 & x_2 \\ \vdots & \vdots \\ 1 & x_n \end{pmatrix} \quad \text{and} \quad \mathbf{y} = \begin{pmatrix} y_1 \\ y_2 \\ \vdots \\ y_n\end{pmatrix}\]</span></p>

<p>We construct <span  class="math">\(\mathbf{X}\)</span> in the code block below. It's important to include the <span  class="math">\(x_0 = 1\)</span> column for the bias (intercept).</p>
<div class="highlight"><pre class="chroma">X = np.hstack((np.ones_like(x), x))
print(X)</pre></div>
<p>Although we can express <span  class="math">\(\mathbf{w}^\star\)</span> explicitly in terms of the matrix inverse <span  class="math">\((\mathbf{X}^\top \mathbf{X})^{-1}\)</span>, this isn't an efficient way to compute <span  class="math">\(\mathbf{w}\)</span> numerically. It is better instead to solve the following system of linear equations</p>

<p><span  class="math">\[\mathbf{X}^\top\mathbf{X} \mathbf{w}^\star = \mathbf{X}^\top\mathbf{y}\]</span></p>

<p>This can be done in numpy using the function linalg.solve</p>
<div class="highlight"><pre class="chroma">w = np.linalg.solve(np.dot(X.T, X), np.dot(X.T, y))
print(w)</pre></div>
<p>Plotting this solution</p>
<div class="highlight"><pre class="chroma">w0, w1 = w
plot_fit(x_test, predict(x_test, w0, w1), x, y)</pre></div>
<p><figure><img src="https://raw.githubusercontent.com/ZintrulCre/zintrulcre.github.io/master/data/Machine-Learning/Machine-Learning-Algorithm-Implementation-1/4.png" alt="4"></figure></p>

<p>We should verify that the sum of squared residuals <span  class="math">\(SSR(w_0, w_1)\)</span>, matches or beats the earlier iterative result.</p>
<div class="highlight"><pre class="chroma">print(compute_SSR(x, y, w0, w1))</pre></div>
<p>The error we computed above is the <strong>training error</strong>. It doesn't assess the model's generalization ability, it only assesses how well it's performing on the given training data. We can assess the generalization ability of models using held-out evaluation data.</p>

<h2 id="6-verification-using-scikitlearn">6. Verification using scikit-learn</h2>

<p>Now we can use the functionality in <code>sklearn</code> to solve linear regression problems to do the verification.</p>

<p>Use the <code>LinearRegression</code> module to fit a linear regression model.</p>
<div class="highlight"><pre class="chroma">from sklearn.linear_model import LinearRegression
lr = LinearRegression().fit(x, y)

lr.intercept_ # bias weight

lr.coef_ # weights

lr.score(x,y)</pre></div>
  </article>

  <br/>

  
      <div id="disqus_thread"></div>
<script type="application/javascript">
    var disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "zintrulcre" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
  
  
</section>

      </div>
      
        
<script>renderMathInElement(document.body);</script>
      
    </main>

    
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA-132809676-1', 'auto');
	
	ga('send', 'pageview');
}
</script>


  <script src="http://zintrulcre.vip/js/app.js"></script>
  
  </body>
</html>
