<!DOCTYPE html>
<html lang="en">

  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <meta name="author" content="Zhengyu Chen">
    <meta name="description" content="ZintrulCre @ 尾張">
    <meta name="keywords" content="ZintrulCre, 尾張">

    <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="并行计算入门"/>
<meta name="twitter:description" content="并行计算入门 [TOC] 1 概述 1.1 并行计算 高性能计算（High Performance Computing）是计算机科学中的一个领域，其目的可以概括为优化性能，它包括了缓存技术"/>

    <meta property="og:title" content="并行计算入门" />
<meta property="og:description" content="并行计算入门 [TOC] 1 概述 1.1 并行计算 高性能计算（High Performance Computing）是计算机科学中的一个领域，其目的可以概括为优化性能，它包括了缓存技术" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://zintrulcre.vip/posts/parallel-computing/parallel-computing/" />
<meta property="article:published_time" content="2020-08-12T18:32:52+08:00" />
<meta property="article:modified_time" content="2020-08-12T18:32:52+08:00" />


    
      <base href="https://zintrulcre.vip/posts/parallel-computing/parallel-computing/">
    
    <title>
  并行计算入门 · 尾張
</title>

    
      <link rel="canonical" href="https://zintrulcre.vip/posts/parallel-computing/parallel-computing/">
    

    <link href="https://fonts.googleapis.com/css?family=Lato:400,700%7CMerriweather:300,700%7CSource+Code+Pro:400,700" rel="stylesheet">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.3.1/css/all.css" integrity="sha384-mzrmE5qonljUremFsqc01SB46JvROS7bZs3IO2EmfFsd15uHvIt+Y8vEf7N7fWAU" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.0/normalize.min.css" integrity="sha256-oSrCnRYXvHG31SBifqP2PM1uje7SJUyX0nTwO2RJV54=" crossorigin="anonymous" />

    
      
      
      <link rel="stylesheet" href="https://zintrulcre.vip/css/coder.min.28d751104f30c16da1aa1bb04015cbe662cacfe0d1b01af4f2240ad58580069c.css" integrity="sha256-KNdREE8wwW2hqhuwQBXL5mLKz&#43;DRsBr08iQK1YWABpw=" crossorigin="anonymous" media="screen" />
    

    

    

    

    <link rel="icon" type="image/png" href="https://zintrulcre.vip/images/favicon-32x32.png" sizes="32x32">
    <link rel="icon" type="image/png" href="https://zintrulcre.vip/images/favicon-16x16.png" sizes="16x16">

    <meta name="generator" content="Hugo 0.75.1" />
  </head>

  <body class=" ">
    <main class="wrapper">
      <nav class="navigation">
  <section class="container">
    <a class="navigation-title" href="https://zintrulcre.vip">
      尾張
    </a>
    <input type="checkbox" id="menu-toggle" />
    <label class="menu-button float-right" for="menu-toggle"><i class="fas fa-bars"></i></label>
    <ul class="navigation-list">
      
        
          <li class="navigation-item">
            <a class="navigation-link" href="https://zintrulcre.vip/posts/">Posts</a>
          </li>
        
          <li class="navigation-item">
            <a class="navigation-link" href="https://zintrulcre.vip/about/">About</a>
          </li>
        
      
      
    </ul>
  </section>
</nav>


      <div class="content">
        
  <section class="container post">
    <article>
      <header>
        <div class="post-title">
          <h1 class="title">并行计算入门</h1>
        </div>
        <div class="post-meta">
          <div class="date">
            <span class="posted-on">
              <i class="fas fa-calendar"></i>
              <time datetime='2020-08-12T18:32:52&#43;08:00'>
                August 12, 2020
              </time>
            </span>
            
          </div>
          <div class="categories">
  <i class="fas fa-folder"></i>
    <a href="https://zintrulcre.vip/categories/parallel-computing/">Parallel Computing</a></div>

          
        </div>
      </header>

      <div>
        <h1 id="并行计算入门">并行计算入门</h1>
<p>[TOC]</p>
<h2 id="1-概述">1 概述</h2>
<h3 id="11-并行计算">1.1 并行计算</h3>
<p><strong>高性能计算</strong>（High Performance Computing）是计算机科学中的一个领域，其目的可以概括为<strong>优化性能</strong>，它包括了缓存技术、数据结构和算法、IO 优化、指令重组（instruction reorganization）、编译器优化等；</p>
<p><strong>并行计算</strong>（Parallel Computing）是<strong>高性能计算</strong>下的一个细分领域，其主要思想是将复杂问题分解成若干个部分，将每一个部分交给独立的处理器（计算资源）进行计算，以提高效率；针对不同的问题，并行计算需要专用的并行架构，架构既可以是专门设计的，含有多个处理器的单一硬件或超级计算机，也可以是以某种方式互连的若干台的独立计算机构成的集群；并没有一个统一的并行计算架构适用于每一个问题，如果使用了错误的架构，并行计算甚至会导致性能下降。</p>
<h3 id="12-硬件架构">1.2 硬件架构</h3>
<p><strong>中央处理器</strong>（Central Processing Unit）的主要功能是解释计算机指令，它由<strong>控制单元</strong>（Control Unit）、<strong>算术逻辑单元</strong>（Arithmetic Logic Unit）、<strong>乱序控制单元</strong>（Out-of-Order Control Unit）、<strong>分支预测器</strong>（Branch Predictor）、<strong>数据缓存</strong>（Data Cache）等部件组成；<strong>CPU</strong> 被设计为可以快速地处理各种通用计算任务并最小化延迟，但在并发性（时钟频率）方面受到限制；</p>
<p><img src="https://raw.githubusercontent.com/ZintrulCre/storage/master/resource/parallel-computing/cpu.png" alt="cpu"></p>
<p><strong>图形处理器</strong>（Graphics Processing Unit, GPU）是英伟达（NVIDIA）在 1999 年 8 月发布 <a href="https://zh.wikipedia.org/wiki/NVIDIA_GeForce_256">NVIDIA GeForce 256</a> 时提出的概念；现代 GPU 的模型设计可以概括为几个关键点：</p>
<ol>
<li>
<p>GPU 的设计目的是最大化吞吐量（Throughput）</p>
</li>
<li>
<p>能够将程序中数据可并行的部分从 CPU 转移到 GPU</p>
</li>
<li>
<p>能够使用尽可能多的线程进行并行计算</p>
</li>
</ol>
<p><strong>GPU</strong> 拥有的<strong>内核</strong>数量相较于 CPU 多得多，可以有数千个同时运行的内核执行大规模并行计算，因此在早期专门应用于图形数据的处理，但随着近十几年的发展，其强大的并行处理能力也使其可以处理非图形数据，尤其在<strong>深度学习</strong>领域非常受欢迎；</p>
<p><img src="https://raw.githubusercontent.com/ZintrulCre/storage/master/resource/parallel-computing/gpu.png" alt="gpu"></p>
<p>在制造工艺的限制下，芯片的密度和最大面积都是有限的（<a href="%5Bhttps://zh.wikipedia.org/wiki/%E6%91%A9%E5%B0%94%E5%AE%9A%E5%BE%8B%5D(https://zh.wikipedia.org/wiki/%E6%91%A9%E5%B0%94%E5%AE%9A%E5%BE%8B)">摩尔定律</a>），因此芯片设计实际上是功能和元件数量的权衡；出于对通用性的要求，<strong>CPU</strong> 的芯片设计必须使用较多种类的原件以增加其功能，同时放弃部分具有复杂功能的元件数量，而 <strong>GPU</strong> 的芯片设计则是通过移除部分具有复杂功能的元件来换取更多的空间，并集成更多的基本功能元件；</p>
<p><img src="https://raw.githubusercontent.com/ZintrulCre/storage/master/resource/parallel-computing/process.png" alt="process"></p>
<p><strong>GPU</strong> 设备由多个<strong>流多处理器</strong>（Streaming Multiprocessor）的<strong>处理器集群</strong>（Processor Cluster）组成。每个<strong>流多处理器</strong>都关联一个<strong>控制单元</strong> 和 L1 Cache，这样的设计使得一个芯片可以同时支持上百个指令流的并行执行；通常一个<strong>流多处理器</strong>在与全局 GDDR-5 内存交换数据之前都会利用与之关联 L1 Cache 和 L2 Cache 来减少数据传输的延迟；而又因为 <strong>GPU</strong> 通常拥有足够大的计算量，使得其不需要与 <strong>CPU</strong> 一样非常频繁地从内存中获取数据，因此 <strong>GPU</strong> 的缓存层一般是小于 <strong>CPU</strong> 的。</p>
<p><img src="https://raw.githubusercontent.com/ZintrulCre/storage/master/resource/parallel-computing/cpu-gpu.png" alt="cpu-gpu"></p>
<p>与 CPU 相比，GPU 可以使用较少且相对较小的内存缓存层。原因是 GPU 具有更多的专用于计算的晶体管，这意味着它无需担心从内存中获取数据需要多长时间。只要 GPU 拥有足够的计算量，就可以掩盖潜在的内存访问 “等待时间”，从而使其保持繁忙状态。</p>
<p><img src="https://raw.githubusercontent.com/ZintrulCre/storage/master/resource/parallel-computing/gpu-vs-cpu.png" alt="gpu-vs-cpu"></p>
<h2 id="2-概念">2 概念</h2>
<h3 id="21-访存模型">2.1 访存模型</h3>
<p>共享内存模型的的计算机中通常有非常多的内核，每个内核都有本地的处理器和缓存；相对的，在互联网络上或其它结点中的处理器和存储一般称为全局的；根据不同的互联网络和访问存储器的方式，一个共享内存机器可以被分为以下几类：</p>
<ol>
<li>
<p>Uniform Memory Access</p>
<p><strong>均匀存储访问</strong>（Uniform Memory Access, UMA）模型的特点是所有的处理器都拥有本地的高速缓存（L1 Cache, L2 Cache），所有的处理器都均匀地共享物理存储（Memory），并且每一个处理器访问任何存储字都需要相同的时间。</p>
<p><img src="https://raw.githubusercontent.com/ZintrulCre/storage/master/resource/parallel-computing/uma.png" alt="uma"></p>
</li>
<li>
<p>Non-Uniform Memory Access</p>
<p><strong>非均匀存储访问</strong>（Non-Uniform Memory Access, NUMA）模型的共享存储器在物理上是分布式的，所有的本地存储器构成了全局地址空间；处理器在访问本地存储器时的速度比访问全局存储器（共享存储器，或其他处理器的本地存储器）快，处理器访问内存的时间取决于内存相对于处理器的位置。</p>
<p>不同的处理器访问共享存储器时，位置的不同会导致访问延迟。</p>
</li>
<li>
<p>Cache-Only Memory Architecture</p>
<p><strong>高速缓存存储结构</strong>（Cache-Only Memory Architecture, COMA）是将 NUMA 中的分布存储器换成了高速缓存，每个处理器上没有存储层次结构，所有的高速缓存共同构成了全局地址空间。</p>
<p><img src="https://raw.githubusercontent.com/ZintrulCre/storage/master/resource/parallel-computing/numa.png" alt="numa"></p>
</li>
</ol>
<h3 id="22-flynn-分类法">2.2 Flynn 分类法</h3>
<p><strong>Flynn 分类法</strong>（Flynn&rsquo;s Taxonomy）是一种高效能计算机的分类方式，他根据指令和数据的执行方式将计算机系统分成了四类：</p>
<ol>
<li>
<p><strong>单指令单数据模型</strong>（Single Instruction Single Data, SISD）</p>
<p>一般来说具有单核 CPU （不讨论超线程技术）的计算机就是基于单指令单数据模型的，对于每一个 CPU 时钟，CPU 按照 <strong>Fetch</strong>（从寄存器中获取数据），<strong>Decode</strong>（解码），<strong>Execute</strong>（执行并将结果保存在另一个寄存器中）的步骤顺序执行指令；上个世纪的计算机几乎都是 SISD 模型的。</p>
</li>
<li>
<p><strong>单指令多数据模型</strong>（Single Instruction Multi Data, SIMD）</p>
<p>单个控制单元拥有多个处理器，这些处理器上运行的线程共享同一个指令流，实现了时间上的并行；<strong>GPU</strong> 就是典型的 SIMD 模型。</p>
</li>
<li>
<p><strong>多指令单数据模型</strong>（Multi Instruction Single Data, MISD）</p>
<p>多个处理器分别拥有自己的控制单元并共享同一个内存单元，应用场景较少。</p>
</li>
<li>
<p><strong>多指令多数据模型</strong>（Multi Instruction Multi Data, MIMD）</p>
<p>多个控制单元异步地控制多个处理器，同时处理器可以在不同的数据上运行不同的程序，一般通过线程或进程层面的并行来实现，从而实现空间上的并行。</p>
</li>
</ol>
<h3 id="23-加速比">2.3 加速比</h3>
<ol>
<li>
<p>加速比</p>
<p><strong>加速比</strong>（Speedup）用于衡量我们现在使用的并行算法比串行算法快了多少，也就是将程序并行化之后提升的效率，其公式是：</p>
<p><img src="https://raw.githubusercontent.com/ZintrulCre/storage/master/resource/parallel-computing/speedup.svg" alt="speedup"></p>
<p>其中 p 代表 CPU 数量，T_1 代表使用串行算法的执行时间，T_p 代表当有 p 个处理器时使用并行算法的执行时间；当 S_p == p ，即 T_1 == p * T_p 时，S_p 称为<strong>线性加速比</strong>（Linear Speedup）。</p>
</li>
<li>
<p>阿姆达尔定律</p>
<p><img src="https://raw.githubusercontent.com/ZintrulCre/storage/master/resource/parallel-computing/amdahls-law.svg" alt="amdahls-law"></p>
<p><strong>阿姆达尔定律</strong>（Amdahl&rsquo;s law）用于估计程序可以达到的最大加速比，W_s 和 W_p 分别表示程序串行部分和并行部分所占的百分比，W_s + W_p 表示程序串行执行的时间（此时并行部分 W_p 相当于被单个处理器执行），W_s + W_p/p 表示程序使用 p 个处理器执行的时间；当 p -&gt; ∞ 时，其上限是 (W_s + W_p) / W_s。</p>
<div class="highlight"><pre style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-c++" data-lang="c++"><span style="color:#fff;font-weight:bold">for</span> (<span style="color:#fff;font-weight:bold">int</span> i = <span style="color:#ff0;font-weight:bold">0</span>; i &lt; <span style="color:#ff0;font-weight:bold">1000000000</span>; ++i) std::this_thread::sleep_for(std::chrono::seconds(<span style="color:#ff0;font-weight:bold">1</span>));   <span style="color:#007f7f">// sequential
</span><span style="color:#007f7f"></span><span style="color:#fff;font-weight:bold">for</span> (<span style="color:#fff;font-weight:bold">int</span> i = <span style="color:#ff0;font-weight:bold">0</span>; i &lt; <span style="color:#ff0;font-weight:bold">1000000000</span>; ++i) std::this_thread::sleep_for(std::chrono::seconds(<span style="color:#ff0;font-weight:bold">1</span>));   <span style="color:#007f7f">// parallel
</span></code></pre></div><p><img src="https://raw.githubusercontent.com/ZintrulCre/storage/master/resource/parallel-computing/amdahl's-law.png" alt="amdahl"></p>
</li>
<li>
<p>古斯塔夫森定律</p>
<p><strong>古斯塔夫森定律</strong>（Gustafson&rsquo;s Law）通过使用 来描述加速比，p 代表处理器的数量，a 代表程序串行化的部分；</p>
<p><img src="https://raw.githubusercontent.com/ZintrulCre/storage/master/resource/parallel-computing/gustafson.png" alt="gustafson"></p>
<p>阿姆达尔定律描述的是增加处理起的数量并不一定能提高加速比，只有增加程序并行部分的比例，才能提高加速比。</p>
<p>古斯塔夫森定律描述的是随着程序并行化比例的提高，加速比与处理器个数成正比的比例（斜率）也在增加。</p>
</li>
<li>
<p>性能</p>
<p><strong>性能</strong>（Efficiency）是由加速比派生出的量度性能的指标，它可以表示每个处理器的加速比，即每个处理器在这个算法中的利用率，其公式是：</p>
<p><img src="https://raw.githubusercontent.com/ZintrulCre/storage/master/resource/parallel-computing/efficiency.svg" alt="efficiency"></p>
</li>
<li>
<p>时钟加速比</p>
<p>S(p) = t_s / t_p</p>
<p><strong>时钟加速比</strong>（Speedup in Wall-Clock Time）的公式很简单，用使用串行算法花费的时钟时间除以使用并行算法花费的时钟时间即可，但是因为时钟时间包括了网络延迟，IO，缓存争用等无关因素，所以它与加速比和算法的复杂度并不相关，只能用于粗略地衡量加速比。</p>
</li>
</ol>
<h2 id="3-并行计算框架">3 并行计算框架</h2>
<h3 id="31-openmp">3.1 OpenMP</h3>
<p><strong>OpenMP</strong>（Open Multi-Processing）是一套针对多处理器<strong>共享内存</strong>机器进行多线程并行编程的 API，支持的语言有 C，C++ 和 Fortran，支持的编译器有现在主流的 GCC 和 Clang 等；</p>
<p>OpenMP 提供了用于描述并行编程的高层抽象，使用 OpenMP 最大的好处在于，当我们没有在编译的时候加上 OpenMP 相关的选项，或当编译器不支持 OpenMp 时，程序仍然可以完成编译，并使用串行的流程正常地运行；这在很大程度上降低了并行编程的难度，使得我们可以把更多的精力投入到并行算法本身，而非其实现细节；尤其对基于数据集进行并行划分的程序，OpenMP是一个很好的选择。</p>
<h4 id="directive">Directive</h4>
<p>所有的 OpenMP 编程操作都是基于 #pragma omp 宏指令（directive）的，每个 directive 都会被转换为与其相应的 OpenMP 库函数调用，而 OpenMP 会处理与线程线程调用相关的操作，包括线程的 fork, join, synchronizing 等，下面是一个简单的例子：</p>
<div class="highlight"><pre style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-c++" data-lang="c++"><span style="color:#0f0;font-weight:bold">#include</span> <span style="color:#0f0;font-weight:bold">&lt;omp.h&gt;</span><span style="color:#0f0;font-weight:bold">
</span><span style="color:#0f0;font-weight:bold">#include</span> <span style="color:#0f0;font-weight:bold">&lt;iostream&gt;</span><span style="color:#0f0;font-weight:bold">
</span><span style="color:#0f0;font-weight:bold"></span> 
<span style="color:#fff;font-weight:bold">int</span> main()
{
   <span style="color:#0f0;font-weight:bold">#pragma omp parallel
</span><span style="color:#0f0;font-weight:bold"></span>   {
       <span style="color:#fff;font-weight:bold">int</span> tid{ omp_get_thread_num() };
       printf(<span style="color:#0ff;font-weight:bold">&#34;Hello world from thread %d</span><span style="color:#0ff;font-weight:bold">\n</span><span style="color:#0ff;font-weight:bold">&#34;</span>, tid);

       <span style="color:#fff;font-weight:bold">int</span> thread_num{ omp_get_num_threads() };
       <span style="color:#fff;font-weight:bold">if</span> (tid == thread_num - <span style="color:#ff0;font-weight:bold">1</span>)
       {
           printf(<span style="color:#0ff;font-weight:bold">&#34;tid: %d, thread_num: %d</span><span style="color:#0ff;font-weight:bold">\n</span><span style="color:#0ff;font-weight:bold">&#34;</span>, tid, thread_num);
       }
   }
    <span style="color:#fff;font-weight:bold">return</span> <span style="color:#ff0;font-weight:bold">0</span>;
}
</code></pre></div><p>注意链接的时候需要加上 -fopenmp，这是一个高层级的标志，其作用主要是链接 gomp 库（GCC 的 OpenMP 实现，如果使用 clang 进行编译则会链接 llvm 对应的实现，类似于 libstdc++ 和 libc++ 的区别），OpenMP 通常是基于 pthread 实现的，所以 gomp 库还会链接更多的库来使用操作系统的线程功能：</p>
<div class="highlight"><pre style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">[joelzychen@DevCloud ~/parallel-computing]$ g++ -std=c++11 -g -o openmp-case openmp-case.cpp -fopenmp
[joelzychen@DevCloud ~/parallel-computing]$ ./openmp-case 
Hello world from thread <span style="color:#ff0;font-weight:bold">5</span>
Hello world from thread <span style="color:#ff0;font-weight:bold">2</span>
Hello world from thread <span style="color:#ff0;font-weight:bold">1</span>
Hello world from thread <span style="color:#ff0;font-weight:bold">3</span>
Hello world from thread <span style="color:#ff0;font-weight:bold">7</span>
tid: 7, thread_num: <span style="color:#ff0;font-weight:bold">8</span>
Hello world from thread <span style="color:#ff0;font-weight:bold">4</span>
Hello world from thread <span style="color:#ff0;font-weight:bold">0</span>
Hello world from thread <span style="color:#ff0;font-weight:bold">6</span>
</code></pre></div><p>omp_get_thread_num() 和 omp_get_num_threads() 两个函数的名称非常直白，分别获取了当前线程的 ID（这个 ID 是 OpenMP 管理的，并不是 PID）和总的线程数；#pragma omp parallel 是最基本的 directive，它可以启动一组线程并让他们并行地执行，如果我们没有在使用 #pragma omp parallel 这个 directive 的时候指定线程数量，那么默认会启动等同于 CPU 核心数量的线程数；并且由于程序是并行地执行的，所以我们并不能保证程序执行的顺序；</p>
<h4 id="example">Example</h4>
<p>再看一个例子：</p>
<div class="highlight"><pre style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-c++" data-lang="c++"><span style="color:#0f0;font-weight:bold">#include</span> <span style="color:#0f0;font-weight:bold">&lt;omp.h&gt;</span><span style="color:#0f0;font-weight:bold">
</span><span style="color:#0f0;font-weight:bold">#include</span> <span style="color:#0f0;font-weight:bold">&lt;iostream&gt;</span><span style="color:#0f0;font-weight:bold">
</span><span style="color:#0f0;font-weight:bold">#include</span> <span style="color:#0f0;font-weight:bold">&lt;thread&gt;</span><span style="color:#0f0;font-weight:bold">
</span><span style="color:#0f0;font-weight:bold">#include</span> <span style="color:#0f0;font-weight:bold">&lt;chrono&gt;</span><span style="color:#0f0;font-weight:bold">
</span><span style="color:#0f0;font-weight:bold"></span>
<span style="color:#fff;font-weight:bold">constexpr</span> <span style="color:#fff;font-weight:bold">int</span> thread_num = <span style="color:#ff0;font-weight:bold">3</span>;
<span style="color:#fff;font-weight:bold">using</span> <span style="color:#fff;font-weight:bold">namespace</span> std; 

<span style="color:#fff;font-weight:bold">int</span> main()
{
    std::chrono::steady_clock::time_point time_begin = std::chrono::steady_clock::now();

<span style="color:#0f0;font-weight:bold">#pragma omp parallel for schedule(static) num_threads(thread_num)
</span><span style="color:#0f0;font-weight:bold"></span>    <span style="color:#fff;font-weight:bold">for</span> (<span style="color:#fff;font-weight:bold">int</span> i = <span style="color:#ff0;font-weight:bold">0</span>; i &lt; thread_num; ++i)    std::this_thread::sleep_for(std::chrono::seconds(<span style="color:#ff0;font-weight:bold">1</span>));

    std::chrono::steady_clock::time_point time_end = std::chrono::steady_clock::now();
    cout &lt;&lt; <span style="color:#0ff;font-weight:bold">&#34;time: &#34;</span> &lt;&lt; std::chrono::duration_cast&lt;std::chrono::milliseconds&gt;(time_end - time_begin).count() &lt;&lt; <span style="color:#0ff;font-weight:bold">&#34; ms&#34;</span> &lt;&lt; endl;

    <span style="color:#fff;font-weight:bold">return</span> <span style="color:#ff0;font-weight:bold">0</span>;
}
</code></pre></div><div class="highlight"><pre style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">[joelzychen@DevCloud ~/parallel-computing]$ g++ -std=c++11 -g -o openmp-case openmp-case.cpp -fopenmp
[joelzychen@DevCloud ~/parallel-computing]$ ./openmp-case 
time: <span style="color:#ff0;font-weight:bold">1000</span> ms
[joelzychen@DevCloud ~/parallel-computing]$ g++ -std=c++11 -g -o openmp-case openmp-case.cpp
[joelzychen@DevCloud ~/parallel-computing]$ ./openmp-case 
time: <span style="color:#ff0;font-weight:bold">3000</span> ms
</code></pre></div><p>因为时间粒度只精确到了毫秒级，所以只能够看到大致的运行时间是 1s 和 3s；<code>num_threads(thread_num)</code> 用于指定线程数量，<code>schedule(static)</code> 用于指定将 <code>for</code> 循环中的迭代以静态的方式分配给多个线程，假设有 n 次循环迭代，t 个线程，那么将给每个线程静态地分配 n/t 次迭代进行运算。</p>
<p>OpenMP 还提供了 barrier（等待所有线程执行完前面的所有计算），atomic（原子操作），flash（写入内存）等各种操作；关于 OpenMP 所有的函数和 directive 可以参考 <a href="https://www.openmp.org/wp-content/uploads/OpenMP-4.5-1115-CPP-web.pdf">OpenMP 4.5 API C/C++ Syntax Reference Guide</a>。</p>
<h3 id="32-openmpi">3.2 OpenMPI</h3>
<p><strong>OpenMPI</strong> (Open Message Passing Interface) 是基于消息队列进行进程间通信的并行编程库，MPI 是一个跨语言的通信协议，OpenMPI 只是遵循这种协议的一种实现；在基于消息队列的并行编程模型中，每个进程都有一个独立的地址空间，一个进程不能直接访问其他进程中的数据，而只能通过消息传递的方式来实现进程间的通信，我们需要显式地通过发送和接受消息来实现处理器之间的数据交换；使用消息队列进行通信的开销比共享内存大，因此它主要用来进行大粒度并行编程的开发。</p>
<h4 id="api">API</h4>
<p>MPI 有几个最基础的函数，在每一个 MPI 并行程序中几乎都会用到这几个函数：</p>
<ol>
<li>
<p><code>int MPI_Init (int* argc ,char** argv[] )</code></p>
<p>初始化 MPI 环境，一般是第一个被调用的 MPI 函数；</p>
</li>
<li>
<p><code>int MPI_Finalize (void)</code></p>
<p>终止 MPI 环境，一般是最后一个被调用的 MPI 函数；</p>
</li>
<li>
<p><code>int MPI_Comm_size (MPI_Comm comm ,int* size )</code></p>
<p>获取通信组进程的个数，<code>MPI_Comm comm</code> 是指定的 communicator，共享通信空间的一组进程组成了通信组，通信组中的所有进程由 communicator 管理；</p>
</li>
<li>
<p><code>int MPI_Comm_rank (MPI_Comm comm ,int* rank)</code></p>
<p>获取当前进程在通信组中的进程 ID，这个 ID 是由 communicator 管理的，不是 PID；</p>
</li>
</ol>
<h4 id="example-1">Example</h4>
<p>来看一个例子，尝试用 OpenMPI 解决 0-1 背包问题，假设物品的数量是 N，背包的容量是 C，第 i 个物品的重量是 weight[i]，价值为 value[i]，先用常规的线性 DP 解决：</p>
<div class="highlight"><pre style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-c++" data-lang="c++"><span style="color:#0f0;font-weight:bold">#include</span> <span style="color:#0f0;font-weight:bold">&lt;cstring&gt;</span><span style="color:#0f0;font-weight:bold">
</span><span style="color:#0f0;font-weight:bold">#include</span> <span style="color:#0f0;font-weight:bold">&lt;iostream&gt;</span><span style="color:#0f0;font-weight:bold">
</span><span style="color:#0f0;font-weight:bold">#include</span> <span style="color:#0f0;font-weight:bold">&lt;fstream&gt;</span><span style="color:#0f0;font-weight:bold">
</span><span style="color:#0f0;font-weight:bold">#include</span> <span style="color:#0f0;font-weight:bold">&lt;vector&gt;</span><span style="color:#0f0;font-weight:bold">
</span><span style="color:#0f0;font-weight:bold">#include</span> <span style="color:#0f0;font-weight:bold">&lt;thread&gt;</span><span style="color:#0f0;font-weight:bold">
</span><span style="color:#0f0;font-weight:bold">#include</span> <span style="color:#0f0;font-weight:bold">&lt;chrono&gt;</span><span style="color:#0f0;font-weight:bold">
</span><span style="color:#0f0;font-weight:bold">#include</span> <span style="color:#0f0;font-weight:bold">&lt;omp.h&gt;</span><span style="color:#0f0;font-weight:bold">
</span><span style="color:#0f0;font-weight:bold"></span>
<span style="color:#fff;font-weight:bold">using</span> <span style="color:#fff;font-weight:bold">namespace</span> std;

<span style="color:#fff;font-weight:bold">int</span> main()
{
    std::chrono::steady_clock::time_point time_begin = std::chrono::steady_clock::now();
    
    fstream input_file(<span style="color:#0ff;font-weight:bold">&#34;input-knapsack.txt&#34;</span>);
    <span style="color:#fff;font-weight:bold">int</span> N;
    <span style="color:#fff;font-weight:bold">int64_t</span> Capacity;
    input_file &gt;&gt; N &gt;&gt; Capacity;
    <span style="color:#fff;font-weight:bold">int64_t</span> weight[N], value[N];
    <span style="color:#fff;font-weight:bold">for</span> (<span style="color:#fff;font-weight:bold">int</span> i = <span style="color:#ff0;font-weight:bold">0</span>; i &lt; N; ++i)
        input_file &gt;&gt; weight[i] &gt;&gt; value[i];
    

    vector&lt;vector&lt;<span style="color:#fff;font-weight:bold">int64_t</span>&gt;&gt; dp(N + <span style="color:#ff0;font-weight:bold">1</span>, vector&lt;<span style="color:#fff;font-weight:bold">int64_t</span>&gt;(Capacity + <span style="color:#ff0;font-weight:bold">1</span>));
    <span style="color:#fff;font-weight:bold">for</span> (<span style="color:#fff;font-weight:bold">int</span> i = <span style="color:#ff0;font-weight:bold">0</span>; i &lt;= N; ++i)
    {
        <span style="color:#0f0;font-weight:bold">#pragma omp parallel for
</span><span style="color:#0f0;font-weight:bold"></span>        <span style="color:#fff;font-weight:bold">for</span> (<span style="color:#fff;font-weight:bold">int64_t</span> j = <span style="color:#ff0;font-weight:bold">0</span>; j &lt;= Capacity; ++j)
        {
            <span style="color:#fff;font-weight:bold">if</span> (i == <span style="color:#ff0;font-weight:bold">0</span> || j == <span style="color:#ff0;font-weight:bold">0</span>)
                dp[i][j] = <span style="color:#ff0;font-weight:bold">0</span>;
            <span style="color:#fff;font-weight:bold">else</span> <span style="color:#fff;font-weight:bold">if</span> (j &lt; weight[i - <span style="color:#ff0;font-weight:bold">1</span>])
                dp[i][j] = dp[i - <span style="color:#ff0;font-weight:bold">1</span>][j];
            <span style="color:#fff;font-weight:bold">else</span>
                dp[i][j] = max(dp[i - <span style="color:#ff0;font-weight:bold">1</span>][j], dp[i - <span style="color:#ff0;font-weight:bold">1</span>][j - weight[i - <span style="color:#ff0;font-weight:bold">1</span>]] + value[i - <span style="color:#ff0;font-weight:bold">1</span>]);
        }
    }
    cout &lt;&lt; <span style="color:#0ff;font-weight:bold">&#34;max value: &#34;</span> &lt;&lt; dp[N][Capacity] &lt;&lt; endl;

    std::chrono::steady_clock::time_point time_end = std::chrono::steady_clock::now();
    cout &lt;&lt; <span style="color:#0ff;font-weight:bold">&#34;time: &#34;</span> &lt;&lt; std::chrono::duration_cast&lt;std::chrono::milliseconds&gt;(time_end - time_begin).count() &lt;&lt; endl;
    <span style="color:#fff;font-weight:bold">return</span> EXIT_SUCCESS;
}
</code></pre></div><p>先用背包数据随机生成器（见附录）生成数据，然后编译运行，对比一下使用 OpenMPI 前后的结果：</p>
<div class="highlight"><pre style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">[joelzychen@DevCloud ~/parallel-computing]$ g++ -std=c++11 -g -o knapsack-generator knapsack-generator.cpp -lcrypto
[joelzychen@DevCloud ~/parallel-computing]$ ./knapsack-generator <span style="color:#ff0;font-weight:bold">1000</span> <span style="color:#ff0;font-weight:bold">8000</span>
[joelzychen@DevCloud ~/parallel-computing]$ g++ -std=c++11 -g -o knapsack knapsack.cpp 
[joelzychen@DevCloud ~/parallel-computing]$ ./knapsack
max value: <span style="color:#ff0;font-weight:bold">13093</span>
time: <span style="color:#ff0;font-weight:bold">175</span> ms
[joelzychen@DevCloud ~/parallel-computing]$ g++ -std=c++11 -g -o knapsack-openmp knapsack.cpp -fopenmp
[joelzychen@DevCloud ~/parallel-computing]$ ./knapsack-openmp
max value: <span style="color:#ff0;font-weight:bold">13093</span>
time: <span style="color:#ff0;font-weight:bold">75</span>
</code></pre></div><p>现在使用 OpenMPI 来改造 0-1 背包问题的 DP 解法：</p>
<div class="highlight"><pre style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-c++" data-lang="c++"><span style="color:#0f0;font-weight:bold">#include</span> <span style="color:#0f0;font-weight:bold">&lt;cstring&gt;</span><span style="color:#0f0;font-weight:bold">
</span><span style="color:#0f0;font-weight:bold">#include</span> <span style="color:#0f0;font-weight:bold">&lt;iostream&gt;</span><span style="color:#0f0;font-weight:bold">
</span><span style="color:#0f0;font-weight:bold">#include</span> <span style="color:#0f0;font-weight:bold">&lt;fstream&gt;</span><span style="color:#0f0;font-weight:bold">
</span><span style="color:#0f0;font-weight:bold">#include</span> <span style="color:#0f0;font-weight:bold">&lt;vector&gt;</span><span style="color:#0f0;font-weight:bold">
</span><span style="color:#0f0;font-weight:bold">#include</span> <span style="color:#0f0;font-weight:bold">&lt;thread&gt;</span><span style="color:#0f0;font-weight:bold">
</span><span style="color:#0f0;font-weight:bold">#include</span> <span style="color:#0f0;font-weight:bold">&lt;chrono&gt;</span><span style="color:#0f0;font-weight:bold">
</span><span style="color:#0f0;font-weight:bold">#include</span> <span style="color:#0f0;font-weight:bold">&lt;mpi.h&gt;</span><span style="color:#0f0;font-weight:bold">
</span><span style="color:#0f0;font-weight:bold"></span>
<span style="color:#fff;font-weight:bold">using</span> <span style="color:#fff;font-weight:bold">namespace</span> std;

<span style="color:#fff;font-weight:bold">int</span> main(<span style="color:#fff;font-weight:bold">int</span> argc, <span style="color:#fff;font-weight:bold">char</span> *argv[])
{
    std::chrono::steady_clock::time_point time_begin = std::chrono::steady_clock::now();

    MPI_Init(&amp;argc, &amp;argv);
    MPI_Comm comm = MPI_COMM_WORLD;
    <span style="color:#fff;font-weight:bold">int</span> rank, size;
    MPI_Comm_rank(comm, &amp;rank);
    MPI_Comm_size(comm, &amp;size);
    MPI_Status status;      <span style="color:#007f7f">// MPI receive
</span><span style="color:#007f7f"></span>    MPI_Request request;    <span style="color:#007f7f">// MPI send
</span><span style="color:#007f7f"></span>    
    fstream input_file(<span style="color:#0ff;font-weight:bold">&#34;input-knapsack.txt&#34;</span>);
    <span style="color:#fff;font-weight:bold">int</span> N;
    <span style="color:#fff;font-weight:bold">int64_t</span> Capacity;
    <span style="color:#fff;font-weight:bold">if</span> (rank == <span style="color:#ff0;font-weight:bold">0</span>)
        input_file &gt;&gt; N &gt;&gt; Capacity;
    MPI_Bcast(&amp;N, <span style="color:#ff0;font-weight:bold">1</span>, MPI_INT, <span style="color:#ff0;font-weight:bold">0</span>, comm);
    MPI_Bcast(&amp;Capacity, <span style="color:#ff0;font-weight:bold">1</span>, MPI_LONG, <span style="color:#ff0;font-weight:bold">0</span>, comm);
    MPI_Barrier(comm);

    <span style="color:#fff;font-weight:bold">int64_t</span> weight[N], value[N];
    <span style="color:#fff;font-weight:bold">if</span> (rank == <span style="color:#ff0;font-weight:bold">0</span>)
        <span style="color:#fff;font-weight:bold">for</span> (<span style="color:#fff;font-weight:bold">int</span> i = <span style="color:#ff0;font-weight:bold">0</span>; i &lt; N; ++i)
            input_file &gt;&gt; weight[i] &gt;&gt; value[i];
    MPI_Bcast(weight, N, MPI_LONG, <span style="color:#ff0;font-weight:bold">0</span>, comm);
    MPI_Bcast(value, N, MPI_LONG, <span style="color:#ff0;font-weight:bold">0</span>, comm);
    MPI_Barrier(comm);


    vector&lt;vector&lt;<span style="color:#fff;font-weight:bold">int64_t</span>&gt;&gt; dp(N + <span style="color:#ff0;font-weight:bold">1</span>, vector&lt;<span style="color:#fff;font-weight:bold">int64_t</span>&gt;(Capacity + <span style="color:#ff0;font-weight:bold">1</span>));
    <span style="color:#fff;font-weight:bold">int64_t</span> prev_max_value;    <span style="color:#007f7f">// mpi send and receive variable
</span><span style="color:#007f7f"></span>    
    <span style="color:#fff;font-weight:bold">for</span> (<span style="color:#fff;font-weight:bold">int</span> i = <span style="color:#ff0;font-weight:bold">0</span>; i &lt;= N; ++i)    <span style="color:#007f7f">// for each item from 0 to n
</span><span style="color:#007f7f"></span>    {
        <span style="color:#fff;font-weight:bold">for</span> (<span style="color:#fff;font-weight:bold">int64_t</span> j = rank; j &lt;= Capacity; j += size)   <span style="color:#007f7f">// for each capacity from 0 to Capacity, each thread computes its own rows
</span><span style="color:#007f7f"></span>        {
            <span style="color:#fff;font-weight:bold">if</span> (i == <span style="color:#ff0;font-weight:bold">0</span> || j == <span style="color:#ff0;font-weight:bold">0</span>)
                dp[i][j] = <span style="color:#ff0;font-weight:bold">0</span>;
            <span style="color:#fff;font-weight:bold">else</span> <span style="color:#fff;font-weight:bold">if</span> (j &lt; weight[i - <span style="color:#ff0;font-weight:bold">1</span>])
                dp[i][j] = dp[i - <span style="color:#ff0;font-weight:bold">1</span>][j];
            <span style="color:#fff;font-weight:bold">else</span>
            {
                <span style="color:#007f7f">// int MPI_Recv(void *buf, int count, MPI_Datatype datatype, int source, int tag, MPI_Comm comm, MPI_Status *status)
</span><span style="color:#007f7f"></span>                MPI_Recv(&amp;prev_max_value, <span style="color:#ff0;font-weight:bold">1</span>, MPI_LONG, (j - weight[i - <span style="color:#ff0;font-weight:bold">1</span>]) % size, i - <span style="color:#ff0;font-weight:bold">1</span>, comm, &amp;status);
                dp[i][j] = max(dp[i - <span style="color:#ff0;font-weight:bold">1</span>][j], prev_max_value + value[i - <span style="color:#ff0;font-weight:bold">1</span>]);
            }

            <span style="color:#007f7f">// send dp[i][j] to the next nodes that may need this curr_max_value
</span><span style="color:#007f7f"></span>            <span style="color:#fff;font-weight:bold">if</span> (i &lt; N &amp;&amp; weight[i] + j &lt;= Capacity)
            {
                <span style="color:#007f7f">// int MPI_Isend(const void *buf, int count, MPI_Datatype datatype, int dest, int tag, MPI_Comm comm, MPI_Request *request)
</span><span style="color:#007f7f"></span>                MPI_Isend(&amp;dp[i][j], <span style="color:#ff0;font-weight:bold">1</span>, MPI_LONG, (j + weight[i]) % size, i, comm, &amp;request);    <span style="color:#007f7f">// asynchronous operation
</span><span style="color:#007f7f"></span>            }
        }
        MPI_Barrier(MPI_COMM_WORLD);
    }
    MPI_Barrier(MPI_COMM_WORLD);
    
    <span style="color:#fff;font-weight:bold">if</span> (rank == Capacity % size)
        printf(<span style="color:#0ff;font-weight:bold">&#34;max value: %ld</span><span style="color:#0ff;font-weight:bold">\n</span><span style="color:#0ff;font-weight:bold">&#34;</span>, dp[N][Capacity]);

    <span style="color:#fff;font-weight:bold">if</span> (rank == <span style="color:#ff0;font-weight:bold">0</span>)
    {
        std::chrono::steady_clock::time_point time_end = std::chrono::steady_clock::now();
        printf(<span style="color:#0ff;font-weight:bold">&#34;time: %ld ms</span><span style="color:#0ff;font-weight:bold">\n</span><span style="color:#0ff;font-weight:bold">&#34;</span>, std::chrono::duration_cast&lt;std::chrono::milliseconds&gt;(time_end - time_begin).count());
    }
    MPI_Finalize();
    <span style="color:#fff;font-weight:bold">return</span> EXIT_SUCCESS;
}
</code></pre></div><p>相比于 OpenMP，使用 OpenMPI 解决背包问题的过程非常复杂，首先要使用 rank == 0 的线程处理输入，然后将输入的 N, C 和 weight, value 数组都广播给其他线程，之后开始处理 dp 数组，动态规划的步骤分为以下几步：</p>
<ol>
<li>
<p>对于从 0 到 n 的每一件物品 i 串行执行；</p>
</li>
<li>
<p>对于第 j 个线程（j = rank, 0 &lt;= j &lt; size），使其去处理对应的 capacity (capacity == j)，之后让 j += size 处理下一件；</p>
</li>
<li>
<p>i == 0 或 j == 0 时初始化边界为 0；</p>
</li>
<li>
<p>如果 j &lt; weight[i - 1]，此时背包容量小于 weight[i - 1]，那么 dp[i][j] = dp[i - 1][j]；</p>
</li>
<li>
<p>如果 j &lt;= weight[i - 1]，此时背包容量大于等于 weight[i - 1]，那么此时 dp[i][j] = max(dp[i - 1][j], dp[i - 1][j - weight[i - 1]] + value[i - 1])，但是由于容量大小为 j - weight[i - 1] 时的 dp 结果（也就是 dp[i - 1][j - weight[i - 1]]）不一定是由第 j 个线程处理的，所以本地的 dp[i - 1][j - weight[i - 1]] 不一定含有正确的值，所以需要通过 mpi 从第 (j - weight[i - 1]) % size 个线程拿到对应的值（也就是 prev_max_value）之后再做处理；</p>
</li>
<li>
<p>对于下一件物品，当背包容量为 j + weight[i] 时可能会用到当前的 dp 结果，因此需要将 dp[i][j] 发送给处理容量为 j + weight[i] 的第 (j + weight[i]) % size 个线程；</p>
</li>
</ol>
<p>最后处理结果时只需要让处理了 dp[N][Capacity] 的第 rank = Capacity % size 个线程输出即可；注意这个做法会有 bug，如果输入中的第 i 个物品的 weight[i] == 0，那么线程在等待 recv 的时候会从自己这个线程接收一个值，从而导致结果不对；</p>
<p>使用 OpenMPI 前先要从<a href="https://www.open-mpi.org/software/ompi/v4.0/">官方网站</a>下载源码并安装（或者通过 yum 安装），然后使用 mpic++ 进行编译，使用 mpirun 运行：</p>
<div class="highlight"><pre style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">[joelzychen@DevCloud ~/parallel-computing/openmpi]$ sudo find / -name <span style="color:#0ff;font-weight:bold">&#34;mpic++&#34;</span>
/usr/lib64/openmpi/bin/mpic++
/usr/lib64/mpich/bin/mpic++
[joelzychen@DevCloud ~/parallel-computing]$ /usr/lib64/mpich/bin/mpic++ -g -std=c++11 -o knapsack-openmpi knapsack-openmpi.cpp 
[joelzychen@DevCloud ~/parallel-computing]$ /usr/lib64/mpich/bin/mpirun -n <span style="color:#ff0;font-weight:bold">1</span> ./knapsack-openmpi
max value: <span style="color:#ff0;font-weight:bold">13093</span>
time: <span style="color:#ff0;font-weight:bold">6694</span> ms
[joelzychen@DevCloud ~/parallel-computing]$ /usr/lib64/mpich/bin/mpirun -n <span style="color:#ff0;font-weight:bold">2</span> ./knapsack-openmpi
max value: <span style="color:#ff0;font-weight:bold">13093</span>
time: <span style="color:#ff0;font-weight:bold">4863</span> ms
[joelzychen@DevCloud ~/parallel-computing]$ /usr/lib64/mpich/bin/mpirun -n <span style="color:#ff0;font-weight:bold">4</span> ./knapsack-openmpi
max value: <span style="color:#ff0;font-weight:bold">13093</span>
time: <span style="color:#ff0;font-weight:bold">3674</span> ms
[joelzychen@DevCloud ~/parallel-computing]$ /usr/lib64/mpich/bin/mpirun -n <span style="color:#ff0;font-weight:bold">8</span> ./knapsack-openmpi
max value: <span style="color:#ff0;font-weight:bold">13093</span>
time: <span style="color:#ff0;font-weight:bold">2487</span> ms
</code></pre></div><p>横向比较 OpenMPI 和 串行算法，在使用 OpenMPI 做 DP 的时候因为在消息传输中浪费了很多时间，其效率甚至不如串行算法；但如果使用穷举算法解 0-1 背包（在 OJ 里会超时的那种），再用 OpenMPI 优化的话，效率会有非常大的提升，有兴趣的同学可以自己了解一下；纵向比较开启不同线程数量的 OpenMPI 算法，我们在收发消息数量不变的情况下提升了同时进行运算的线程数量，因此明显地提高了效率；关于 OpenMPI 的所有函数可以查阅 <a href="https://www.open-mpi.org/doc/current/">Open MPI v4.0.4 documentation</a>。</p>
<h3 id="33-cuda">3.3 CUDA</h3>
<p>CUDA 的全称是 Compute Unified Device Architecture，它是一个用于并行计算的平台和 API，它允许开发人员使用<strong>支持 CUDA 的 GPU</strong> 进行并行编程；GPU 并不能独立进行运算，它需要与 CPU 通过 PCIe 总线连接到一起协同进行工作，使用 GPU 进行的并行计算可以被视为是 CPU 和 GPU 的异构计算架构，CPU 负责处理逻辑复杂的串行部分，GPU 负责处理数据密集的并行部分，其中 CPU 通常被称为 host 主机端，GPU 通常被称为 device 设备端；</p>
<p><img src="https://raw.githubusercontent.com/ZintrulCre/storage/master/resource/parallel-computing/heterogeneous-computing.png" alt="heterogeneous-computing"></p>
<h4 id="kernel">Kernel</h4>
<p>CUDA 中的 <strong>kernel 核函数</strong>是在 GPU 端并行执行的函数，这个函数只包含程序的并行部分，它会被 GPU 上的诸多线程并行执行；相比于 CPU 上的线程，GPU 上的线程更加轻量级，创建的成本更小，线程切换更灵活，进入 CUDA 核函数时程序可以定义非常多的虚拟线程，但能够并行执行的硬件线程数也是有限的；一般来说基于 CUDA 程序的执行流程如下：</p>
<ol>
<li>host 端进行内存分配和数据初始化，执行程序串行部分</li>
<li>device 端进行内存分配，并从 host 端拷贝数据到 device 端</li>
<li>device 端调用并执行核函数，同时使用缓存提升效率</li>
<li>device 端将运算好的结果拷贝到 host 端上</li>
<li>device 端释放内存，等待下一次核函数调用</li>
</ol>
<p><img src="https://raw.githubusercontent.com/ZintrulCre/storage/master/resource/parallel-computing/simple-process-flow.png" alt="simple-process-flow"></p>
<h4 id="thread-hierarchy">Thread Hierarchy</h4>
<p>CUDA 执行核函数的时候开启的线程拥有三层的层级结构：</p>
<p><img src="https://raw.githubusercontent.com/ZintrulCre/storage/master/resource/parallel-computing/grid-block-thread.png" alt="grid-block-thread"></p>
<ol>
<li>
<p>grid</p>
<p>grid 是一个逻辑实体，可以理解为一个工作区，它运行在整个 GPU 上，同一个 grid 里的所有 thread 共享全局内存空间；</p>
</li>
<li>
<p>thread block</p>
<p>thread block 是一组并行执行的线程，一个 block 在单个 streaming multi-processor 中运行，即一个 block 中的所有 thread 都在这个流式多处理器中运行，它们可以通过共享内存或同步原语进行通信，位于不同的 block 中的 thread 一般来说不能互相通信和协作，每一个 block 都应该能够独立运行；</p>
</li>
<li>
<p>thread</p>
<p>thread 在 CUDA core 上执行，正如前文所说，GPU 上的线程非常轻量级，可以通过较大的寄存器提供非常快速的上下文切换（CPU 的线程句柄存在于较低的内存层次结构中，例如高速缓存）；</p>
</li>
</ol>
<p>我们在调用核函数的时候需要通过 <code>&lt;&lt;&lt;block, thread&gt;&gt;&gt;</code> 的方式来指定 block 和 thread 的数量和维度。</p>
<h4 id="example-2">Example</h4>
<p>来看一个简单的例子，先串行地进行大约 10 亿次加法运算：</p>
<div class="highlight"><pre style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-c++" data-lang="c++"><span style="color:#0f0;font-weight:bold">#include</span> <span style="color:#0f0;font-weight:bold">&lt;iostream&gt;</span><span style="color:#0f0;font-weight:bold">
</span><span style="color:#0f0;font-weight:bold">#include</span> <span style="color:#0f0;font-weight:bold">&lt;cstdlib&gt;</span><span style="color:#0f0;font-weight:bold">
</span><span style="color:#0f0;font-weight:bold">#include</span> <span style="color:#0f0;font-weight:bold">&lt;cstring&gt;</span><span style="color:#0f0;font-weight:bold">
</span><span style="color:#0f0;font-weight:bold">#include</span> <span style="color:#0f0;font-weight:bold">&lt;chrono&gt;</span><span style="color:#0f0;font-weight:bold">
</span><span style="color:#0f0;font-weight:bold">#include</span> <span style="color:#0f0;font-weight:bold">&lt;thread&gt;</span><span style="color:#0f0;font-weight:bold">
</span><span style="color:#0f0;font-weight:bold"></span>
<span style="color:#fff;font-weight:bold">using</span> <span style="color:#fff;font-weight:bold">namespace</span> std;
<span style="color:#fff;font-weight:bold">constexpr</span> <span style="color:#fff;font-weight:bold">uint64_t</span> magic_number = <span style="color:#ff0;font-weight:bold">12345</span>;

<span style="color:#fff;font-weight:bold">void</span> Add(<span style="color:#fff;font-weight:bold">int</span> n, <span style="color:#fff;font-weight:bold">uint64_t</span> *x)
{
    <span style="color:#fff;font-weight:bold">for</span> (<span style="color:#fff;font-weight:bold">int</span> i = <span style="color:#ff0;font-weight:bold">0</span>; i &lt; n; ++i)
        x[i] += x[i];
}

<span style="color:#fff;font-weight:bold">int</span> main(<span style="color:#fff;font-weight:bold">void</span>)
{
    <span style="color:#fff;font-weight:bold">int</span> n = <span style="color:#ff0;font-weight:bold">1</span>&lt;&lt;<span style="color:#ff0;font-weight:bold">30</span>;
    <span style="color:#fff;font-weight:bold">uint64_t</span> *x = (<span style="color:#fff;font-weight:bold">uint64_t</span> *)malloc(n * <span style="color:#fff;font-weight:bold">sizeof</span>(<span style="color:#fff;font-weight:bold">uint64_t</span>));
    memset(x, magic_number, <span style="color:#fff;font-weight:bold">sizeof</span>(x)); 

    std::chrono::steady_clock::time_point time_begin = std::chrono::steady_clock::now();
    Add(n, x);
    std::chrono::steady_clock::time_point time_end = std::chrono::steady_clock::now();
    cout &lt;&lt; <span style="color:#0ff;font-weight:bold">&#34;time: &#34;</span> &lt;&lt; std::chrono::duration_cast&lt;std::chrono::milliseconds&gt;(time_end - time_begin).count() &lt;&lt; <span style="color:#0ff;font-weight:bold">&#34; ms&#34;</span> &lt;&lt; endl;

    free(x);
    <span style="color:#fff;font-weight:bold">return</span> <span style="color:#ff0;font-weight:bold">0</span>;
}
</code></pre></div><p>为了方便对比，在 Windows PowerShell 中使用 nvcc 编译运行：</p>
<div class="highlight"><pre style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-powershell" data-lang="powershell"><span style="color:#fff;font-weight:bold">PS </span>G:\&gt; nvcc -o add .\add.<span style="color:#fff;font-weight:bold">cpp </span> -ccbin <span style="color:#0ff;font-weight:bold">&#34;C:\Program Files (x86)\Microsoft Visual Studio\2017\Professional\VC\Tools\MSVC\14.16.27023\bin\Hostx64\x64&#34;</span>
add.<span style="color:#fff;font-weight:bold">cpp
</span><span style="color:#fff;font-weight:bold">PS </span>G:\&gt; .\add.exe
time<span style="color:#f00">:</span> 4472 ms
</code></pre></div><p>可以看到 Add 函数串行执行的时间大约是 4472 ms，现在我们将其修改为使用 CUDA 的并行程序：</p>
<div class="highlight"><pre style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-c++" data-lang="c++"><span style="color:#0f0;font-weight:bold">#include</span> <span style="color:#0f0;font-weight:bold">&lt;iostream&gt;</span><span style="color:#0f0;font-weight:bold">
</span><span style="color:#0f0;font-weight:bold">#include</span> <span style="color:#0f0;font-weight:bold">&lt;cstdlib&gt;</span><span style="color:#0f0;font-weight:bold">
</span><span style="color:#0f0;font-weight:bold">#include</span> <span style="color:#0f0;font-weight:bold">&lt;cstring&gt;</span><span style="color:#0f0;font-weight:bold">
</span><span style="color:#0f0;font-weight:bold">#include</span> <span style="color:#0f0;font-weight:bold">&lt;chrono&gt;</span><span style="color:#0f0;font-weight:bold">
</span><span style="color:#0f0;font-weight:bold">#include</span> <span style="color:#0f0;font-weight:bold">&lt;thread&gt;</span><span style="color:#0f0;font-weight:bold">
</span><span style="color:#0f0;font-weight:bold">#include</span> <span style="color:#0f0;font-weight:bold">&lt;string&gt;</span><span style="color:#0f0;font-weight:bold">
</span><span style="color:#0f0;font-weight:bold"></span>
<span style="color:#fff;font-weight:bold">using</span> <span style="color:#fff;font-weight:bold">namespace</span> std;
<span style="color:#fff;font-weight:bold">constexpr</span> <span style="color:#fff;font-weight:bold">int</span> magic_number = <span style="color:#ff0;font-weight:bold">12345</span>;

__global__ <span style="color:#fff;font-weight:bold">void</span> Add(<span style="color:#fff;font-weight:bold">int</span> n, <span style="color:#fff;font-weight:bold">int</span> *x)
{
    <span style="color:#fff;font-weight:bold">for</span> (<span style="color:#fff;font-weight:bold">int</span> i = <span style="color:#ff0;font-weight:bold">0</span>; i &lt; n; ++i)
        x[i] += x[i];
}

<span style="color:#fff;font-weight:bold">int</span> main(<span style="color:#fff;font-weight:bold">void</span>)
{
    <span style="color:#fff;font-weight:bold">int</span> n = <span style="color:#ff0;font-weight:bold">1</span>&lt;&lt;<span style="color:#ff0;font-weight:bold">30</span>;
    <span style="color:#fff;font-weight:bold">int64_t</span> byte_size = n * <span style="color:#fff;font-weight:bold">sizeof</span>(<span style="color:#fff;font-weight:bold">int</span>);
    <span style="color:#fff;font-weight:bold">int</span> *x;
    x = (<span style="color:#fff;font-weight:bold">int</span>*)malloc(byte_size);
    <span style="color:#fff;font-weight:bold">for</span> (<span style="color:#fff;font-weight:bold">int</span> i = <span style="color:#ff0;font-weight:bold">0</span>; i &lt; n; ++i)
        x[i] = magic_number;
    
    <span style="color:#fff;font-weight:bold">int</span> *cuda_x;
    cudaMalloc((<span style="color:#fff;font-weight:bold">void</span>**)&amp;cuda_x, byte_size);

    <span style="color:#007f7f">// copy from host to device
</span><span style="color:#007f7f"></span>    cudaMemcpy(cuda_x, x, byte_size, cudaMemcpyHostToDevice);

    std::chrono::steady_clock::time_point time_begin = std::chrono::steady_clock::now();
    Add&lt;&lt;&lt;<span style="color:#ff0;font-weight:bold">1</span>, <span style="color:#ff0;font-weight:bold">1</span>&gt;&gt;&gt;(n, cuda_x);
    cudaDeviceSynchronize();
    std::chrono::steady_clock::time_point time_end = std::chrono::steady_clock::now();
    
    <span style="color:#007f7f">// copy from device to host
</span><span style="color:#007f7f"></span>    cudaMemcpy(x, cuda_x, byte_size, cudaMemcpyDeviceToHost);

    <span style="color:#007f7f">// check result
</span><span style="color:#007f7f"></span>    <span style="color:#fff;font-weight:bold">bool</span> result{ <span style="color:#fff;font-weight:bold">true</span> };
    <span style="color:#fff;font-weight:bold">for</span> (<span style="color:#fff;font-weight:bold">uint32_t</span> i = <span style="color:#ff0;font-weight:bold">0</span>; i &lt; n; ++i)
        result = (result &amp;&amp; (x[i] == magic_number + magic_number));
    string result_str = (result ? <span style="color:#0ff;font-weight:bold">&#34;true&#34;</span> : <span style="color:#0ff;font-weight:bold">&#34;false&#34;</span>);

    cout &lt;&lt; <span style="color:#0ff;font-weight:bold">&#34;result: &#34;</span> &lt;&lt; result_str &lt;&lt; endl;
    cout &lt;&lt; <span style="color:#0ff;font-weight:bold">&#34;time: &#34;</span> &lt;&lt; std::chrono::duration_cast&lt;std::chrono::milliseconds&gt;(time_end - time_begin).count() &lt;&lt; <span style="color:#0ff;font-weight:bold">&#34; ms&#34;</span> &lt;&lt; endl;
    free(x);
    cudaFree(cuda_x);
    <span style="color:#fff;font-weight:bold">return</span> <span style="color:#ff0;font-weight:bold">0</span>;
}
</code></pre></div><div class="highlight"><pre style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-powershell" data-lang="powershell"><span style="color:#fff;font-weight:bold">PS </span>G:\&gt; nvcc -o cuda-add .\cuda-add.cu -ccbin <span style="color:#0ff;font-weight:bold">&#34;C:\Program Files (x86)\Microsoft Visual Studio\2017\Professional\VC\Tools\MSVC\14.16.27023\bin\Hostx64\x64&#34;</span>
cuda-add.cu
   Creating library cuda-add.lib and object cuda-add.exp
<span style="color:#fff;font-weight:bold">PS </span>G:\&gt; .\cuda-add.exe  
result<span style="color:#f00">:</span> true
time<span style="color:#f00">:</span> 21904 ms
</code></pre></div><p>可以看到 CUDA 程序里有一些特殊的关键字和 API：</p>
<ol>
<li>
<p><code>__global__</code> 是核函数的标志，只要在函数签名前加上 <code>__global__</code> 它就可以被 CUDA 编译器分析为核函数；</p>
</li>
<li>
<p>因为 Add 函数是在 device 端运行的，我们需要先使用 <code>malloc</code> 和 <code>cudaMalloc</code> 分别为 host 和 device 端分配内存，然后使用 <code>cudaMemcpy</code> 将在 host 端初始化的数据拷贝到 device 端；</p>
</li>
<li>
<p>我们需要在 device 端调用 Add 核函数，这个操作对于 host 端来说是异步的，它不会等待 device 端的执行结果，我们需要调用 <code>cudaDeviceSynchronize</code> 函数来等待 device 端执行完毕并返回；如果我们连续调用了多个核函数，又没有在 device 端指定控制流，那么这些核函数只会在 device 端按顺序执行；</p>
</li>
<li>
<p>device 端执行完之后，使用 <code>cudaMemcpy</code> 将数据从 device 端拷贝回 host 端进行验证；</p>
</li>
<li>
<p>最后分别调用 <code>free</code> 和 <code>cudaFree</code> 来释放内存；</p>
</li>
</ol>
<p>我们的程序虽然跑在 GPU 上，但速度反而比跑在 CPU 上的时候更慢了，因为我们只为 kernel 分配了 1 个 block 和 1 个 thread (<code>Add&lt;&lt;&lt;1,1&gt;&gt;&gt;(n, x);</code>)，既没有发挥 GPU 并行计算的优势，又浪费了时间在 CPU 和 GPU 的交互上；优化的方法和 OpenMPI 中的例子类似，只需要让每个线程处理对应自己的数据，并在循环中每次自增一个步长：</p>
<div class="highlight"><pre style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-cpp" data-lang="cpp">__global__ <span style="color:#fff;font-weight:bold">void</span> Add(<span style="color:#fff;font-weight:bold">int</span> n, <span style="color:#fff;font-weight:bold">int</span> *cuda_x)
{
    <span style="color:#fff;font-weight:bold">int</span> index = blockIdx.x * blockDim.x + threadIdx.x;
    <span style="color:#fff;font-weight:bold">int</span> stride = blockDim.x;
    <span style="color:#fff;font-weight:bold">for</span> (<span style="color:#fff;font-weight:bold">int</span> i = index; i &lt; n; i += stride)
        cuda_x[i] += cuda_x[i];
}

<span style="color:#fff;font-weight:bold">int</span> main(<span style="color:#fff;font-weight:bold">void</span>)
{
    <span style="color:#007f7f">// ...
</span><span style="color:#007f7f"></span>    Add&lt;&lt;&lt;<span style="color:#ff0;font-weight:bold">4096</span>, <span style="color:#ff0;font-weight:bold">256</span>&gt;&gt;&gt;(n, cuda_x);
    <span style="color:#007f7f">// ...
</span><span style="color:#007f7f"></span>}
</code></pre></div><p>其中：</p>
<ul>
<li>
<p>blockIdx.x 代表 block 的 ID，即当前 block 的下标；</p>
</li>
<li>
<p>blockDim.x 代表 block 的维度，即一个 block 内含有多少个 thread；同时也是步长 stride；</p>
</li>
<li>
<p>类似的，threadIdx.x 代表 thread 的 ID，即当前 block 的下标；</p>
</li>
<li>
<p>index 是当前需要进行运算的数据在内存中的位置</p>
</li>
</ul>
<p>在这个例子中，我们开启了 4096 个 block 和 256 个 thread，即 blockIdx.x &lt; 4096, blockDim.x == 256, threadIdx.x &lt; 256；</p>
<p><img src="https://raw.githubusercontent.com/ZintrulCre/storage/master/resource/parallel-computing/grid-dim.png" alt="grid-dim"></p>
<p>当然如果申请过多的 block 并不会提升运算的效率，因为 cuda core 会浪费许多时间来调度这些 block；我们可以多次修改 <code>&lt;&lt;&lt;block, thread&gt;&gt;&gt;</code> 来对比在使用不同数量的 block 和 thread 的情况下的性能：</p>
<div class="highlight"><pre style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-powershell" data-lang="powershell"><span style="color:#007f7f"># Add&lt;&lt;&lt;4096, 256&gt;&gt;&gt;(n, x);</span>
<span style="color:#fff;font-weight:bold">PS </span>G:\&gt; nvcc -o cuda-add .\cuda-add.cu -ccbin <span style="color:#0ff;font-weight:bold">&#34;C:\Program Files (x86)\Microsoft Visual Studio\2017\Professional\VC\Tools\MSVC\14.16.27023\bin\Hostx64\x64&#34;</span>                                                                                      cuda-add.cu
   Creating library cuda-add.lib and object cuda-add.exp
<span style="color:#fff;font-weight:bold">PS </span>G:\&gt; .\cuda-add.exe
result<span style="color:#f00">:</span> true
time<span style="color:#f00">:</span> 159454 ms

<span style="color:#007f7f"># Add&lt;&lt;&lt;1, 256&gt;&gt;&gt;(n, x);</span>
<span style="color:#fff;font-weight:bold">PS </span>G:\&gt; .\cuda-add.exe
result<span style="color:#f00">:</span> true
time<span style="color:#f00">:</span> 1038 ms

<span style="color:#007f7f"># Add&lt;&lt;&lt;1, 1024&gt;&gt;&gt;(n, x);</span>
<span style="color:#fff;font-weight:bold">PS </span>G:\&gt; .\cuda-add.exe
result<span style="color:#f00">:</span> true
time<span style="color:#f00">:</span> 299 ms
</code></pre></div><p><img src="https://raw.githubusercontent.com/ZintrulCre/storage/master/resource/parallel-computing/cuda-add.gif" alt="cuda-add"></p>
<p>关于 CUDA 的更多使用说明可以参考 <a href="https://docs.nvidia.com/cuda/">CUDA Toolkit Documentation</a>。</p>
<h2 id="4-总结">4 总结</h2>
<p>本文主要讲解了并行计算所依附的硬件架构及其相关的一些概念，通过 OpenMP, OpenMPI 和 CUDA 分别简单地介绍了基于共享内存，消息传递和 GPU（其实也是一种共享内存并行编程）三种方法的并行编程，关于并行计算的更多开发经验还需要在实践中积累。</p>
<p>本文所有代码均收录在 <a href="https://github.com/ZintrulCre/parallel-computing-demo">https://github.com/ZintrulCre/parallel-computing-demo</a>。</p>
<h2 id="5-附录">5 附录</h2>
<h3 id="0-1-背包问题随机数据生成器">0-1 背包问题随机数据生成器</h3>
<div class="highlight"><pre style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-c++" data-lang="c++"><span style="color:#007f7f">// knapsack-generator.cpp
</span><span style="color:#007f7f"></span><span style="color:#0f0;font-weight:bold">#include</span> <span style="color:#0f0;font-weight:bold">&lt;iostream&gt;</span><span style="color:#0f0;font-weight:bold">
</span><span style="color:#0f0;font-weight:bold">#include</span> <span style="color:#0f0;font-weight:bold">&lt;fstream&gt;</span><span style="color:#0f0;font-weight:bold">
</span><span style="color:#0f0;font-weight:bold">#include</span> <span style="color:#0f0;font-weight:bold">&lt;openssl/rand.h&gt;</span><span style="color:#0f0;font-weight:bold">
</span><span style="color:#0f0;font-weight:bold"></span>
<span style="color:#fff;font-weight:bold">using</span> <span style="color:#fff;font-weight:bold">namespace</span> std;

<span style="color:#fff;font-weight:bold">int</span> main (<span style="color:#fff;font-weight:bold">int</span> argc, <span style="color:#fff;font-weight:bold">char</span> *argv[])
{
    <span style="color:#fff;font-weight:bold">if</span> (argc &lt; <span style="color:#ff0;font-weight:bold">3</span>)
    {
        fprintf(stderr, <span style="color:#0ff;font-weight:bold">&#34;usage: %s N C</span><span style="color:#0ff;font-weight:bold">\n</span><span style="color:#0ff;font-weight:bold">&#34;</span>, argv[<span style="color:#ff0;font-weight:bold">0</span>]);
        exit(<span style="color:#ff0;font-weight:bold">1</span>);
    }
    
    <span style="color:#fff;font-weight:bold">int</span> N = stoi (argv[<span style="color:#ff0;font-weight:bold">1</span>]);
    <span style="color:#fff;font-weight:bold">uint64_t</span> C = stoi (argv[<span style="color:#ff0;font-weight:bold">2</span>]);
    
    <span style="color:#fff;font-weight:bold">int</span> m = <span style="color:#ff0;font-weight:bold">4</span> * C / N;
    <span style="color:#fff;font-weight:bold">unsigned</span> <span style="color:#fff;font-weight:bold">char</span> buff[<span style="color:#ff0;font-weight:bold">2</span> * N];
    RAND_seed(&amp;m, <span style="color:#fff;font-weight:bold">sizeof</span>(m));
    RAND_bytes(buff, <span style="color:#fff;font-weight:bold">sizeof</span>(buff));

    ofstream file_stream;
    file_stream.open(<span style="color:#0ff;font-weight:bold">&#34;input-knapsack.txt&#34;</span>);
    file_stream &lt;&lt; N &lt;&lt; <span style="color:#0ff;font-weight:bold">&#39; &#39;</span> &lt;&lt; C &lt;&lt; endl;
    <span style="color:#fff;font-weight:bold">for</span> (<span style="color:#fff;font-weight:bold">int</span> i = <span style="color:#ff0;font-weight:bold">0</span>; i &lt; N; i++)
    {
        file_stream &lt;&lt; buff[<span style="color:#ff0;font-weight:bold">2</span> * i] % m &lt;&lt; <span style="color:#0ff;font-weight:bold">&#39; &#39;</span> &lt;&lt; buff[<span style="color:#ff0;font-weight:bold">2</span> * i + <span style="color:#ff0;font-weight:bold">1</span>] % m &lt;&lt; endl;;
    }
    file_stream.close();

    <span style="color:#fff;font-weight:bold">return</span> <span style="color:#ff0;font-weight:bold">0</span>;
}
</code></pre></div><h3 id="参考文献">参考文献</h3>
<ol>
<li>
<p>OpenMP 4.5 API C/C++ Syntax Reference Guide. (2020). Retrieved 10 August 2020, from <a href="https://www.openmp.org/wp-content/uploads/OpenMP-4.5-1115-CPP-web.pdf">https://www.openmp.org/wp-content/uploads/OpenMP-4.5-1115-CPP-web.pdf</a></p>
</li>
<li>
<p>Open MPI v4.0.4 documentation. (2020). Retrieved 10 August 2020, from <a href="https://www.open-mpi.org/doc/current/">https://www.open-mpi.org/doc/current/</a></p>
</li>
<li>
<p>Jiaoyun, Yang &amp; Yun, Xu &amp; Yi, Shang. (2010). An Efficient Parallel Algorithm for Longest Common Subsequence Problem on GPUs. Lecture Notes in Engineering and Computer Science. 1.</p>
</li>
<li>
<p>CUDA Toolkit Documentation. (2020). Retrieved 10 August 2020, from <a href="https://docs.nvidia.com/cuda/">https://docs.nvidia.com/cuda/</a></p>
</li>
<li>
<p>Harwood, A., &amp; Lanch, A. (2020). COMP90025 Parallel and Multicore. Retrieved 10 August 2020, from School of Computing and Information Systems The University of Melbourne</p>
</li>
<li>
<p>Zeller, C. (2011). CUDA C/C++ Basics Supercomputing. Retrieved 7 August 2020, from <a href="https://www.nvidia.com/docs/IO/116711/sc11-cuda-c-basics.pdf">https://www.nvidia.com/docs/IO/116711/sc11-cuda-c-basics.pdf</a></p>
</li>
<li>
<p>Han, J., &amp; Sharma, B. Learn CUDA programming.</p>
</li>
<li>
<p>Ruetsch, G., &amp; Oster, B. (2020). Getting Started with CUDA. Retrieved 10 August 2020, from <a href="https://www.nvidia.com/content/cudazone/download/Getting_Started_w_CUDA_Training_NVISION08.pdf">https://www.nvidia.com/content/cudazone/download/Getting_Started_w_CUDA_Training_NVISION08.pdf</a></p>
</li>
<li>
<p>Harris, M. (2017). An Even Easier Introduction to CUDA. Retrieved 10 August 2020, from <a href="https://developer.nvidia.com/blog/even-easier-introduction-cuda/">https://developer.nvidia.com/blog/even-easier-introduction-cuda/</a></p>
</li>
<li>
<p>Modern Parallel Computing (Part 3) - Some Typical GPU Architectures · Infectious Waste. (2020). Retrieved 10 August 2020, from <a href="https://infectiouswaste.github.io/2019/02/20/typical-gpu-arch/">https://infectiouswaste.github.io/2019/02/20/typical-gpu-arch/</a></p>
</li>
<li>
<p>Cheng, J. (2014). Professional Cuda C programming. Indianapolis, IN: John Wiley and Sons, Inc.</p>
</li>
<li>
<p>Harris, M., Ebersole, M., &amp; Sakharnykh, N. (2020). Unified Memory in CUDA 6 | NVIDIA Developer Blog. Retrieved 10 August 2020, from <a href="https://developer.nvidia.com/blog/unified-memory-in-cuda-6/">https://developer.nvidia.com/blog/unified-memory-in-cuda-6/</a></p>
</li>
</ol>

      </div>

      <footer>
        


        <div id="disqus_thread"></div>
<script type="application/javascript">
    var disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "zintrulcre" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
      </footer>
    </article>

    
  </section>

      </div>

      <footer class="footer">
  <section class="container">
    
    
    
    
      
      <a href="https://github.com/luizdepra/hugo-coder/tree/"></a>
    
  </section>
</footer>

<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_SVG"></script>
    </main>

    
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA-132809676-1', 'auto');
	
	ga('send', 'pageview');
}
</script>


  </body>

</html>
