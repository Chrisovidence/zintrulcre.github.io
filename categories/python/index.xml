<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Python on 尾張</title>
    <link>http://zintrulcre.github.io/categories/python/</link>
    <description>Recent content in Python on 尾張</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 04 Aug 2021 17:38:52 +0800</lastBuildDate><atom:link href="http://zintrulcre.github.io/categories/python/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Python 源码学习（5）：协程</title>
      <link>http://zintrulcre.github.io/posts/python/python-source-code-coroutine/</link>
      <pubDate>Wed, 04 Aug 2021 17:38:52 +0800</pubDate>
      
      <guid>http://zintrulcre.github.io/posts/python/python-source-code-coroutine/</guid>
      <description>Python 源码学习（5）：协程 协程 coroutine 是一种用户态的轻量级线程，它可以在函数的特定位置暂停或恢复，同时调用者可以从协程中获取状态或将状态传递给协程；Python中的生成器 generator 就是一个典型的协程应用，本文简单地对 Python 中生成器的实现进行分析。
1 生成器 如果 Python 中的函数含有 yield 关键字，那么在调用这个函数时，它不会如同普通的函数一样运行到 return 语句并返回一个变量，而是会立即返回一个生成器对象；以一个斐波那契数列生成函数为例：
def FibonacciSequenceGenerator(): a, b = 0, 1 while True: yield a + b a, b = b, a + b if __name__ == &amp;#34;__main__&amp;#34;: fsg = FibonacciSequenceGenerator() print(fsg) print(type(fsg)) $ python3 main.py &amp;lt;generator object FibonacciSequenceGenerator at 0x7fb4720b1ac0&amp;gt; &amp;lt;class &amp;#39;generator&amp;#39;&amp;gt; 可以看到函数 FibonacciSequenceGenerator 返回了一个类型为 generator 的生成器对象 f；对于生成器对象，我们不能像操作普通函数一样直接进行函数调用，而是要使用 next() 或 fsg.send() 来进行函数切换，使得生成器函数开始或继续执行，直到 yield 所在行或是函数末尾再将执行权交还给调用方：
for i in range(100): print(next(fsg)) $ python3 main.</description>
    </item>
    
    <item>
      <title>Python 源码学习（4）：编译器和虚拟机</title>
      <link>http://zintrulcre.github.io/posts/python/python-source-code-interpreter/</link>
      <pubDate>Wed, 26 May 2021 10:18:52 +0800</pubDate>
      
      <guid>http://zintrulcre.github.io/posts/python/python-source-code-interpreter/</guid>
      <description>Python 源码学习（4）：编译器和虚拟机 Python 是一种解释型语言，一般在使用前我们会从 Python 官方网站上下载使用 C 语言开发编译的 CPython 解释器，本文用到的源码均来自 CPython。
Python 解释器（Python Interpreter）由 Python 编译器（Python Compiler）和 Python 虚拟机（Python Virutal Machine）两部分组成。当我们通过 Python 命令执行 Python 代码时，Python 编译器会将 Python 代码编译为 Python 字节码（bytecode）；随后 Python 虚拟机会读取并逐步执行这些字节码。
1 Python 编译器 1.1 代码对象 Python 提供了内置函数 compile，可以编译 Python 代码并生成一个包含字节码信息的对象，举例如下：
# test.py def Square(a): return a * a print(f&amp;#34;result:\t\t{Square(5)}&amp;#34;) # main.py f = &amp;#34;test.py&amp;#34; code_obj = compile(open(f).read(), f, &amp;#39;exec&amp;#39;) exec(code_obj) print(f&amp;#34;code_obj:\t{code_obj}&amp;#34;) print(f&amp;#34;type:\t\t{type(code_obj)}&amp;#34;) $ python3 main.py result: 25 code_obj: &amp;lt;code object &amp;lt;module&amp;gt; at 0x7f052c156b30, file &amp;#34;test.</description>
    </item>
    
    <item>
      <title>Python 源码学习（3）：list 类型</title>
      <link>http://zintrulcre.github.io/posts/python/source-code-3-list-and-dict/</link>
      <pubDate>Thu, 06 May 2021 20:07:52 +0800</pubDate>
      
      <guid>http://zintrulcre.github.io/posts/python/source-code-3-list-and-dict/</guid>
      <description>Python 源码学习（3）：list 类型 Python 中的 list 类型在源码中是一个名为 PyListObject 的结构体，定义在 listobject.h 文件中：
// Include/cpython/listobject.h typedef struct { PyObject_VAR_HEAD /* Vector of pointers to list elements. list[0] is ob_item[0], etc. */ PyObject **ob_item; /* ob_item contains space for &amp;#39;allocated&amp;#39; elements. The number * currently in use is ob_size. * Invariants: * 0 &amp;lt;= ob_size &amp;lt;= allocated * len(list) == ob_size * ob_item == NULL implies ob_size == allocated == 0 * list.sort() temporarily sets allocated to -1 to detect mutations.</description>
    </item>
    
    <item>
      <title>Python 源码学习（2）：int 类型</title>
      <link>http://zintrulcre.github.io/posts/python/source-code-2/</link>
      <pubDate>Wed, 31 Mar 2021 15:37:52 +0800</pubDate>
      
      <guid>http://zintrulcre.github.io/posts/python/source-code-2/</guid>
      <description>Python 源码学习（2）：int 类型 Python 中的标准数据类型有六种，分别是 number, string, list, tuple, set, dictionary，前文已经阐述过它们的对象类型都是继承了 PyBaseObject_Type 类型的 PyType_Type 类型的实例对象，本文则主要探究 Python 中 int 类型的实现。
不同于 C 和 C++ 中的 int 类型，Python 中的 int 类型最大的特点是它一般是不会溢出的，对比用 C 和 Python 分别输出两个一百万相乘的结果：
&amp;gt;&amp;gt;&amp;gt; x = 10000000000 &amp;gt;&amp;gt;&amp;gt; print(x) 10000000000 在 C 语言中会发生溢出：
printf(&amp;#34;%d\n&amp;#34;, 1000000 * 1000000); printf(&amp;#34;%u\n&amp;#34;, 1000000 * 1000000); -727379968 3567587328 1 int 类型在内存中的存储方式 1.1 内存结构 Python 中的 int 整数类型实际上是一个名为 PyLongObject 的结构体，定义在 longintrepr.h 文件中：
// Include/object.h #define PyObject_VAR_HEAD PyVarObject ob_base;  // Objects/longobject.</description>
    </item>
    
    <item>
      <title>Python 源码学习（1）：类型和对象</title>
      <link>http://zintrulcre.github.io/posts/python/source-code-1/</link>
      <pubDate>Sun, 14 Mar 2021 16:05:52 +0800</pubDate>
      
      <guid>http://zintrulcre.github.io/posts/python/source-code-1/</guid>
      <description>Python 源码学习（1）：类型和对象 Python 是一门解释型，动态类型，多范式的编程语言，当我们从 python.org 下载并安装运行 Python 的某个分发版本时，我们实际上是在运行由 C 语言编写的 CPython，除此之外 Python 的运行时还有 Jython, PyPy, Cython 等；CPython 的源码中有一系列的库，组件和工具：
$ git clone https://github.com/python/cpython $ tree -d -L 2 . . `-- cpython |-- Doc	# 文档 |-- Grammar |-- Include # C 头文件 |-- Lib	# 用 Python 写的库文件 |-- Mac	# 用于在 macOS 上构建的文件 |-- Misc	# 杂项 |-- Modules	# 用 C 写的库文件 |-- Objects # 核心类型，以及对象模型的定义 |-- PC	# 用于在 Windows 上构建的文件 |-- PCbuild # 用于在老版本的 Windows 上构建的文件 |-- Parser	# Python 解析器源码 |-- Programs	# Python 可执行文件和其他 |-- Python	# CPython 编译器源码 |-- Tools	# 构建时的工具 `-- m4 16 directories 本系列主要以阅读和分析 CPython 源码的方式学习 Python。</description>
    </item>
    
    <item>
      <title>LeetCode Archiver(3)： 登录</title>
      <link>http://zintrulcre.github.io/posts/leetcode-archiver/leetcode-archiver3/</link>
      <pubDate>Fri, 11 Jan 2019 13:15:17 +1100</pubDate>
      
      <guid>http://zintrulcre.github.io/posts/leetcode-archiver/leetcode-archiver3/</guid>
      <description>Cookie和Session 为了获取我们自己的提交记录，我们首先要进行登录的操作。但我们都知道HTTP是一种无状态的协议，它的每个请求都是独立的。无论是GET还是POST请求，都包含了处理当前这一条请求的所有信息，但它并不会涉及到状态的变化。因此，为了在无状态的HTTP协议上维护一个持久的状态，引入了Cookie和Session的概念，两者都是为了辨识用户相关信息而储存在内存或硬盘上的加密数据。
Cookie是由客户端浏览器维护的。客户端浏览器会在需要时把Cookie保存在内存中，当其再次向该域名相关的网站发出request时，浏览器会把url和Cookie一起作为request的一部分发送给服务器。服务器通过解析该Cookie来确认用户的状态，并对Cookie的内容作出相应的修改。一般来说，如果不设置过期时间，非持久Cookie会保存在内存中，浏览器关闭后就被删除了。
Session是由服务器维护的。当客户端第一次向服务器发出request后，服务器会为该客户端创建一个Session。当该客户端再次访问服务器时，服务器会根据该Session来获取相关信息。一般来说，服务器会为Seesion设置一个失效时间，当距离接收到客户端上一次发送request的时间超过这个失效时间后，服务器会主动删除Session。
两种方法都可以用来维护登录的状态。为了简便起见，本项目目前使用Session作为维护登录状态的方法。
获取数据 分析 首先我们进入登录页面，打开开发者工具，勾选Preserve log。为了知道在登录时浏览器向服务器提交了哪些数据，我们可以先输入一个错误的用户名和密码，便于抓包。
通过分析&amp;quot;login/&amp;ldquo;这条request，我们可以知道我们所需要的一些关键信息，例如headers中的user-agent和referer，表单数据（form data）中的csrfmiddlewaretoken，login和password。显然，user-agent和referer我们可以直接复制下来，login和password是我们填写的用户名和密码。还有一个很陌生的csrfmiddlewaretoken。这是CSRF的中间件token，CSRF是Cross-Site Request Forgery，相关知识可以查询跨站请求伪造的维基百科。那么现在我们就要分析这个token是从何而来。
获取csrfmiddlewaretoken 我们将刚才获取到的csrfmiddlewaretoken复制下来，在开发者工具中使用搜索功能，可以发现这个csrfmiddlewaretoken出现在了登录之前的一些request对应的response中。例如在刚才打开登录页面，发送GET请求时，response的headers的set-cookie中出现了&amp;quot;csrftoken=&amp;hellip;&amp;ldquo;，而这里csrftoken的值与我们需要在登录表单中提交的值完全相同。因此，我们可以通过获取刚才的response中的Cookies来获取csrfmiddlewaretoken的值。
首先我们通过发送GET请求来分析一下Cookies的构成
login_url = &amp;quot;https://leetcode.com/accounts/login/&amp;quot; session = requests.session() result = session.get(login_url) print(result) print(type(result.cookies)) for cookie in result.cookies: print(type(cookie)) print(cookie) 得到的结果是
 &amp;lt;Response [200]&amp;gt; 状态码200，表示请求成功 &amp;lt;class &#39;requests.cookies.RequestsCookieJar&#39;&amp;gt; cookies的类型是CookieJar &amp;lt;class &#39;http.cookiejar.Cookie&#39;&amp;gt; 第一条cookie的类型是Cookie &amp;lt;Cookie__cfduid=d3e02d4309b848f9369e21671fabbce571548041181 for .leetcode.com/&amp;gt; 第一条cookie的信息 &amp;lt;class &#39;http.cookiejar.Cookie&#39;&amp;gt; 第二条cookie的类型是Cookie &amp;lt;Cookie csrftoken=13mQWE9tYN6g2IrlKY8oMLRc4VhVNoet4j328YdDapW2WC2nf93y5iCuzorovTDl for leetcode.com/&amp;gt; 第二条cookie的信息，也就是我们所需要的csrftoken  这样一来我们便获取到了在提交表单信息时所需要的csrfmiddlewaretoken，之后我们便可以开始着手写登录的相关代码了。顺便一提，在使用Django进行后端开发的时候自动生成的csrf token的键也叫csrfmiddlewaretoken，不知道LeetCode是不是用Django作为后端开发框架的。
实现 首先我们需要在爬虫开始运行之前获取登录信息，将Session作为类的成员变量保存下来，方便在获取submissions时使用。同时我们需要在与爬虫文件相同的目录下新建config.json，将自己的用户名和密码保存在该json文件里，这样就能顺利登陆了。
 def start_requests(self): self.Login() # 登录 questionset_url = &amp;quot;https://leetcode.com/api/problems/all/&amp;quot; yield scrapy.</description>
    </item>
    
    <item>
      <title>LeetCode Archiver(2)：获取题目信息</title>
      <link>http://zintrulcre.github.io/posts/leetcode-archiver/leetcode-archiver2/</link>
      <pubDate>Fri, 21 Dec 2018 15:06:01 +1100</pubDate>
      
      <guid>http://zintrulcre.github.io/posts/leetcode-archiver/leetcode-archiver2/</guid>
      <description>创建爬虫 在新建好项目后，用PyCharm或其他IDE打开该项目。进入该项目文件夹，使用genspider命令新建一个爬虫：
cd scrapy_project scrapy genspider QuestionSetSpider leetcode.com 其中QuestionSetSpider是爬虫的名字，leetcode.com是我们打算爬取的网站的域名。
新建好爬虫之后可以看到在项目的spiders文件夹下新增了一个名为 QuestionSetSpider.py的文件，这就是我们刚才新建的爬虫文件。这个爬虫文件会自动生成以下代码
# -*- coding: utf-8 -*- import scrapy class QuestionSetSpider(scrapy.Spider): name = &#39;QuestionSetSpider&#39; allowed_domains = [&#39;leetcode.com&#39;] start_urls = [&#39;http://leetcode.com/&#39;] def parse(self, response): pass  QuestionSetSpider类继承自scrapy.Spider，也就是scrapy框架中所有爬虫的基类； self.name属性是该爬虫的名字，在该爬虫文件的外部可以通过这个属性获取当前爬虫； self.allowed_domains是当前爬虫文件可以访问的域名列表，如果在爬取页面时进入了一个该域名以外的url会抛出错误； self.start_urls是一个url列表，基类中定义了start_requests函数，它会遍历self.start_urls，并对每一个url调用scrapy.Request(url, dont_filter=True)，为了实现爬取题目的需求，我们需要重写self.start_urls函数  获取题目详细信息 分析 LeetCode使用了GraphQL进行数据的查询和传输，大部分页面都是通过JS渲染生成的动态页面，所以无法直接从页面上获取标签，即使使用提供JavaScript渲染服务的库（例如Splash）也无法获取全部的数据，所以只能通过发送请求来获取数据。
为了爬取题目的详细信息，我们首先要从题目列表进入每个题目对应的链接。
首先打开leetcode的problem列表，按F12打开Chrome的开发者工具，进入Network标签栏，勾选上Preserve log，刷新该页面。
可以看到，网页向 https://leetcode.com/api/problems/all/ 发送了一个名为&amp;quot;all/&amp;ldquo;的GET类型的Request，这就是获取所有题目链接和相关信息的请求。如果此时已经安装了Toggle JavaScript插件，我们可以直接右键点击“Open in new tab”，查看该请求返回的Response。
更方便的方法是使用postman向服务器发送一个相同的Request，并将其保存下来，这样如果我们下次需要查看相应的Response的时候就不需要再使用开发者工具了。
返回的Response是一个json对象，其中的&amp;quot;stat_status_pairs&amp;quot;键所对应的值是所有包含题目信息的list，而列表中的[&amp;ldquo;stat&amp;rdquo;][&amp;ldquo;question__title_slug&amp;rdquo;]就是题目所在的页面。以Largest Perimeter Triangle为例，将其title_slug拼接到https://leetcode.com/problems/ 后，进入页面https://leetcode.com/problems/largest-perimeter-triangle/ 。同样地，打开开发者工具，刷新页面，可以看到服务器返回了很多项graphql的查询数据，通过查看Request Payload可以找到其中operationName为&amp;quot;questionData&amp;quot;的一项，这就是当前题目的详细信息。
将Payload复制粘贴到postman的Body中，在Headers中设置Content-Type为application/json，发送请求，可以看到返回的是一个json对象，包含了该题目所对应的所有信息。
接下来我们就可以对该题目的信息进行处理了。
实现 为了获取题目列表的json对象，我们需要先重写start_requests函数。
def start_requests(self): self.Login() # 用户登录，后续会用到 questionset_url = &amp;quot;https://leetcode.</description>
    </item>
    
    <item>
      <title>LeetCode Archiver(1)：Scrapy框架和Requests库</title>
      <link>http://zintrulcre.github.io/posts/leetcode-archiver/leetcode-archiver1/</link>
      <pubDate>Tue, 04 Dec 2018 11:25:15 +1100</pubDate>
      
      <guid>http://zintrulcre.github.io/posts/leetcode-archiver/leetcode-archiver1/</guid>
      <description>简介 Scrapy官方文档对Scrapy的介绍如下：
Scrapy是一个为了爬取网站数据，提取结构性数据而编写的应用框架。可以应用在包括数据挖掘，信息处理或存储历史数据等一系列的程序中。其最初是为了页面抓取（更确切来说, 网络抓取）所设计的，也可以应用在获取API所返回的数据（例如 Amazon Associates Web Services ）或者通用的网络爬虫。
简而言之，Scrapy是基于Twisted库开发的，封装了http请求、代理信息、数据存储等功能的Python爬虫框架。
组件和数据流 下图是Scrapy官方文档中的架构概览图：
图中绿色箭头表示数据流，其他均为组件。
Scrapy Engine（引擎） 引擎负责控制数据流在系统的组件中流动，并在相应动作发生时触发事件。
Scheduler（调度器） 调度器从引擎接收request并将其保存，以便在引擎请求时提供给引擎。
Downloader（下载器） 下载器负责下载页面数据，并将其提供给引擎，而后再由引擎提供给爬虫。
Spiders（爬虫） Spider是由用户编写的用于分析response并提取item或额外跟进url的类。一个Scrapy项目中可以有很多Spider，他们分别被用于爬取不同的页面和网站。
Item Pipeline（管道） Item Pipeline负责处理被爬虫提取出来的item。可以对其进行数据清洗，验证和持久化（例如存储到数据库中）。
Downloader middlewares（下载器中间件） 下载器中间件是在引擎及下载器之间的组件，用于处理下载器传递给引擎的response。更多内容请参考下载器中间件。
Spider middlewares（爬虫中间件） Spider中间件是在引擎及Spider之间的组件，用于处理爬虫的输入（response）和输出（items和requests）。更多内容请参考爬虫中间件。
Data flow（数据流） Scrapy中的数据流由引擎控制，其过程如下:1.引擎打开一个网站，找到处理该网站的爬虫并向该爬虫请求要爬取的url。2.引擎从爬虫中获取到要爬取的url并将其作为request发送给调度器。3.引擎向调度器请求下一个要爬取的url。4.调度器返回下一个要爬取的url给引擎，引擎将url通过下载器中间件发送给下载器。5.下载器下载页面成功后，生成一个该页面的response对象，并将其通过下载器中间件发送给引擎。6.引擎接收从下载器中间件发送过来的response，并将其通过爬虫中间件发送给爬虫处理。7.爬虫处理response，并将爬取到的item及跟进的新的request发送给引擎。8.引擎将爬虫返回的item发送给管道，将爬虫返回的新的request发送给调度器。9.管道对item进行相应的处理。10.重复第二步，直到调度器中没有更多的request，此时引擎关闭该网站。安装 1.下载安装最新版的Python3
2.使用pip指令安装Scrapy
pip3 install scrapy 创建项目 首先进入你的代码存储目录，在命令行中输入以下命令：
scrapy startproject LeetCode_Crawler 注意项目名称是不能包含连字符 &amp;lsquo;-&amp;rsquo; 的
新建成功后，可以看到在当前目录下新建了一个名为LeetCode_Crawler的Scrapy项目，进入该目录，其项目结构如下：
scrapy.cfg #该项目的配置文件 scrapy_project #该项目的Python模块 __init__.py items.py #可自定义的item类文件 middlewares.py #中间件文件 pipelines.py #管道文件 settings.py #设置文件 __pycache__ spiders #爬虫文件夹，所有爬虫文件都应在该文件夹下 __init__.py __pycache__ 至此Scrapy项目的创建就完成了。
参考资料 Scrapy官方文档</description>
    </item>
    
  </channel>
</rss>
