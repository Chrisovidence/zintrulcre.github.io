<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Splash on ZintrulCre&#39;s Blog</title>
    <link>http://zintrulcre.top/tags/splash/</link>
    <description>Recent content in Splash on ZintrulCre&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; Copyright 2019 ZintrulCre</copyright>
    <lastBuildDate>Thu, 03 Jan 2019 11:25:15 +1100</lastBuildDate>
    
	<atom:link href="http://zintrulcre.top/tags/splash/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>使用Scrapy和Splash爬取LeetCode（1）：Scrapy框架入门</title>
      <link>http://zintrulcre.top/post/use-scrapy-and-splash-to-crawl-leetcode-1/</link>
      <pubDate>Thu, 03 Jan 2019 11:25:15 +1100</pubDate>
      
      <guid>http://zintrulcre.top/post/use-scrapy-and-splash-to-crawl-leetcode-1/</guid>
      <description>简介 Scrapy官方文档对Scrapy的介绍如下：
Scrapy是一个为了爬取网站数据，提取结构性数据而编写的应用框架。可以应用在包括数据挖掘，信息处理或存储历史数据等一系列的程序中。
其最初是为了页面抓取（更确切来说, 网络抓取）所设计的，也可以应用在获取API所返回的数据（例如 Amazon Associates Web Services ）或者通用的网络爬虫。
简而言之，Scrapy是基于Twisted库开发的，封装了http请求、代理信息、数据存储等功能的Python爬虫框架。
组件和数据流 下图是Scrapy官方文档中的架构概览图：
图中绿色箭头表示数据流，其他均为组件。
Scrapy Engine（引擎） 引擎负责控制数据流在系统的组件中流动，并在相应动作发生时触发事件。
Scheduler（调度器） 调度器从引擎接收request并将其保存，以便在引擎请求时提供给引擎。
Downloader（下载器） 下载器负责下载页面数据，并将其提供给引擎，而后再由引擎提供给爬虫。
Spiders（爬虫） Spider是由用户编写的用于分析response并提取item或额外跟进url的类。一个Scrapy项目中可以有很多Spider，他们分别被用于爬取不同的页面和网站。
Item Pipeline（管道） Item Pipeline负责处理被爬虫提取出来的item。可以对其进行数据清洗，验证和持久化（例如存储到数据库中）。
Downloader middlewares（下载器中间件） 下载器中间件是在引擎及下载器之间的组件，用于处理下载器传递给引擎的response。更多内容请参考下载器中间件。
Spider middlewares（爬虫中间件） Spider中间件是在引擎及Spider之间的组件，用于处理爬虫的输入（response）和输出（items和requests）。更多内容请参考爬虫中间件。
Data flow（数据流） Scrapy中的数据流由引擎控制，其过程如下:
1.引擎打开一个网站，找到处理该网站的爬虫并向该爬虫请求要爬取的url。
2.引擎从爬虫中获取到要爬取的url并将其作为request发送给调度器。
3.引擎向调度器请求下一个要爬取的url。
4.调度器返回下一个要爬取的url给引擎，引擎将url通过下载器中间件发送给下载器。
5.下载器下载页面成功后，生成一个该页面的response对象，并将其通过下载器中间件发送给引擎。
6.引擎接收从下载器中间件发送过来的response，并将其通过爬虫中间件发送给爬虫处理。
7.爬虫处理response，并将爬取到的item及跟进的新的request发送给引擎。
8.引擎将爬虫返回的item发送给管道，将爬虫返回的新的request发送给调度器。
9.管道对item进行相应的处理。
10.重复第二步，直到调度器中没有更多的request，此时引擎关闭该网站。
安装 1.下载安装最新版的 Python3
2.使用pip指令安装Scrapy
 pip3 install scrapy  新建项目 首先进入你的代码存储目录，在命令行中输入以下命令：
scrapy startproject scrapy_project  startproject
新建成功后，可以看到在当前目录下新建了一个名为scrapy_project的Scrapy项目，其项目结构如下：
scrapy.cfg #该项目的配置文件 scrapy_project #该项目的Python模块 __init__.py items.py #可自定义的item类文件 middlewares.</description>
    </item>
    
  </channel>
</rss>